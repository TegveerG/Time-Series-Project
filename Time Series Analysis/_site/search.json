[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Tegveer Ghura",
    "section": "",
    "text": "Tegveer is a first-year graduate student pursuing an MS in Data Science & Analytics at Georgetown University. When not producing insights from data, Tegveer enjoys spending time playing cricket and exploring the DMV."
  },
  {
    "objectID": "Data-Visualization.html",
    "href": "Data-Visualization.html",
    "title": "Data Visualization",
    "section": "",
    "text": "The analysis of the data collected begins from this pivotal section. Every data science project, especially those utilizing Time Series data, starts with Data Visualization because it allows for easy and intuitive interpretation of complex data sets. These visualizations help data scientists identify patterns, trends, and relationships that might otherwise be difficult to discern by looking at the summary statistics the raw data, for example. The visualizations presented below were created using Tableau and the packages ggplot2 and Plotly in the R software.\nCode for this section can be found here"
  },
  {
    "objectID": "Data-Visualization.html#ethereum-eth-line-plots",
    "href": "Data-Visualization.html#ethereum-eth-line-plots",
    "title": "Data Visualization",
    "section": "Ethereum (ETH) Line Plots",
    "text": "Ethereum (ETH) Line Plots\n\n\n\n\nStatic PlotInteractive Plot\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInvestors who bought stocks during the COVID-19 market crash in 2020 have generally experienced some big gains in the past two years. Several factors led to a surge in Ethereum buying in 2020 and especially in 2021. Younger Americans receiving three rounds of direct stimulus payments have poured a significant chunk of that cash into investments, including Ethereum. In addition, cryptocurrency investing became extremely trendy in 2021, and Ethereum was one of the most popular cryptos in the market. At the beginning of 2020, Ethereum was trading around $129. By the beginning of March, the cryptocurrency had risen to $218 as news of the virus spreading in China prompted concerns about a U.S. pandemic. On March 13, 2020, Ethereum plummeted to its pandemic low of $88.50 as global stock markets tanked. The good news for Ethereum investors is the crypto bounced off that level as the stock market began to stabilize shortly thereafter and the government started printing money."
  },
  {
    "objectID": "Data-Visualization.html#ethereum-candlestick-plot",
    "href": "Data-Visualization.html#ethereum-candlestick-plot",
    "title": "Data Visualization",
    "section": "Ethereum Candlestick Plot",
    "text": "Ethereum Candlestick Plot\n\n\n\n\n\n\nTo visualize the candlesticks carefully, we subset the data by viewing only the last few months’ data from the current date, January 15th 2023. Ethereum’s price was generally decreasing since November 2022, experiencing massive drops on November 9th notably. The crypto was trading at a similar level until the hopeful New Year, which then saw its price increase back to former levels in November 2022."
  },
  {
    "objectID": "index.html#education",
    "href": "index.html#education",
    "title": "Tegveer Ghura",
    "section": "Education",
    "text": "Education\nBoston University | Boston, MA\nB.A (Cum Laude) Economics | Sept 2017 - May 2021\nGeorgetown University | Washington, DC\nM.S Data Science & Analytics | Aug 2022 - May 2024 (anticipated)"
  },
  {
    "objectID": "index.html#experience",
    "href": "index.html#experience",
    "title": "Tegveer Ghura",
    "section": "Experience",
    "text": "Experience\nMetricStream, Inc. | Finance Associate | Sept 2021 - Apr 2022\nMetricStream, Inc. | Finance Intern | June 2021 - Aug 2022\nUnimoni Financial Services Ltd. | Project Intern | Jul 2019 - Aug 2019"
  },
  {
    "objectID": "Introduction.html",
    "href": "Introduction.html",
    "title": "Introduction",
    "section": "",
    "text": "Warning\n\n\n\nTrigger warning: The following content contains descriptions of violent acts and extremism related to terrorism, which may be disturbing or triggering for some readers.\n\n\nTerrorism is a puzzling and gripping phenomenon. Its relationship with tourism is intricate and multi-dimensional. Interestingly, international terrorism and tourism share common traits, such as being transnational in nature, involving citizens from different nations, and utilizing travel and communication technologies. The impact of terrorist attacks extends to several other industries related to tourism, including airlines, hotels, restaurants, and tourist-oriented shops and services (Baker, n.d.).\n\nBaker, David. n.d. “The Effects of Terrorism on the Travel and Tourism Industry.” International Journal of Religious Tourism and Pilgrimage. https://arrow.tudublin.ie/cgi/viewcontent.cgi?article=1052&amp;context=ijrtp.\n\n“World Tourism Organization.” n.d. UNWTO. https://www.unwto.org/.\nReceipts from international tourism in destinations around the world grew by 4% in 2012 reaching USD 1,075 billion. This growth is equal to a 4% increase in international tourist arrivals over the previous year which reached 1,035 million in 2012. An additional USD 219 billion was recorded in receipts from international passenger transport, bringing total exports generated by international tourism in 2012 to US$ 1.3 trillion (“World Tourism Organization,” n.d.).\n\n\n\n\n\n\n\n\n\nThe Global Terrorism Database™ (GTD) (“Codebook Methodology Inclusion Criteria and Variables - UMD,” n.d.) defines a terrorist attack as the threatened or actual use of illegal force and violence by a nonstate actor to attain a political, economic, religious, or social goal through fear, coercion, or intimidation. In practice this means in order to consider an incident for inclusion in the GTD (“Codebook Methodology Inclusion Criteria and Variables - UMD,” n.d.), all three of the following attributes must be present:\n\nThe incident must be intentional – the result of a conscious calculation on the part of a perpetrator.\nThe incident must entail some level of violence or immediate threat of violence, including property violence, as well as violence against people.\nThe perpetrators of the incidents must be sub-national actors. The database does not include acts of state terrorism.\n\nIn addition, at least two of the following three criteria must be present for an incident to be included in the GTD (“Codebook Methodology Inclusion Criteria and Variables - UMD,” n.d.):\n\n“Codebook Methodology Inclusion Criteria and Variables - UMD.” n.d. Global Terrorism Database. University of Maryland. https://www.start.umd.edu/gtd/downloads/Codebook.pdf.\n\nCriterion 1: The act must be aimed at attaining a political, economic, religious, or social goal. In terms of economic goals, the exclusive pursuit of profit does not satisfy this criterion. It must involve the pursuit of more profound, systemic economic change.\nCriterion 2: There must be evidence of an intention to coerce, intimidate, or convey some other message to a larger audience (or audiences) than the immediate victims. It is the act taken as a totality that is considered, irrespective if every individual involved in carrying out the act was aware of this intention. As long as any of the planners or decision-makers behind the attack intended to coerce, intimidate or publicize, the intentionality criterion is met.\nCriterion 3: The action must be outside the context of legitimate warfare activities. That is, the act must be outside the parameters permitted by international humanitarian law, insofar as it targets non-combatants"
  },
  {
    "objectID": "Data-Sources.html",
    "href": "Data-Sources.html",
    "title": "Data Sources",
    "section": "",
    "text": "In order to collect time-series data on terrorism, choosing The Global Terrorism Database™ (GTD) by University of Maryland was a pragmatic decision because it contains detailed information of global attacks that occurred daily. This gave enormous flexibility to search for short-term seasonal patterns, if any, regarding terrorist attacks. For assessing the impact of terrorism on economic activity, the SIPRI Military Expenditure Database was employed. Military expenditure as a share of GDP can provide key insights about a country’s allocation of resources. Nonimmigrant Admissions data was also collected from the Department of Homeland Security (DHS) to add an extra dimension of how the sentiment of the US government changed regarding tourist admits in the overall analysis.\nThe Quantmod R package seamlessly allows R users to get stock data, which aids in exploring both changes in prices due to certain terror attacks and how weapons manufacturing companies reacted to these attacks. To analyze domestic financial impacts from terror attacks at a granular level, daily historical stock prices of some of the largest weapons manufacturers in the United States, including Lockheed Martin and Raytheon Technologies, were obtained. Not only stock prices of individual companies, but also data of the Dow Jones U.S. Travel & Tourism Index was gathered to approach the financial analysis at a larger scale."
  },
  {
    "objectID": "Data-Sources.html#the-global-terrorism-database-gtd-by-university-of-maryland",
    "href": "Data-Sources.html#the-global-terrorism-database-gtd-by-university-of-maryland",
    "title": "Data Sources",
    "section": "The Global Terrorism Database™ (GTD) by University of Maryland",
    "text": "The Global Terrorism Database™ (GTD) by University of Maryland\nAn open-source database containing information on terrorist events around the world from 1970 through 2020 (with annual updates planned for the future). Unlike many other event databases, the GTD includes systematic data on domestic as well as international terrorist incidents that have occurred during this time period and now includes more than 200,000 cases.\n\n\n\nPress the logo to access the data!"
  },
  {
    "objectID": "Data-Sources.html#sipri-military-expenditure-database",
    "href": "Data-Sources.html#sipri-military-expenditure-database",
    "title": "Data Sources",
    "section": "SIPRI Military Expenditure Database",
    "text": "SIPRI Military Expenditure Database\nThe SIPRI Military Expenditure Database contains consistent time series on the military spending of countries for the period 1949–2021. The database is updated annually, which may include updates to data for any of the years included in the database. The main purpose of the data on military expenditure is to provide an easily identifiable measure of the scale of resources absorbed by the military. Military expenditure is an input measure which is not directly related to the ‘output’ of military activities, such as military capability or military security. Military expenditure data measured in constant dollars is a trend indicator of the volume of resources used for military activities, which allow comparisons to be made over time for individual countries and between countries.\n\n\n\nPress the logo to access the data!"
  },
  {
    "objectID": "Data-Sources.html#department-of-homeland-security-dhs",
    "href": "Data-Sources.html#department-of-homeland-security-dhs",
    "title": "Data Sources",
    "section": "Department of Homeland Security (DHS)",
    "text": "Department of Homeland Security (DHS)\nThe United States Department of Homeland Security (DHS) is the U.S. federal executive department responsible for public security, roughly comparable to the interior or home ministries of other countries. Its stated missions involve anti-terrorism, border security, immigration and customs, cyber security, and disaster prevention and management.\n\n\n\nPress the logo to access the data!"
  },
  {
    "objectID": "Data-Sources.html#quantmod-r-package",
    "href": "Data-Sources.html#quantmod-r-package",
    "title": "Data Sources",
    "section": "Quantmod R Package",
    "text": "Quantmod R Package\nThe quantmod package for R is designed to assist the quantitative trader in the development, testing, and deployment of statistically based trading models. The getSymbols() function in the package aids in collecting stock price data of US companies and indexes. As previously mentioned, the historical stock prices of Lockheed Martin and the Dow Jones U.S. Travel & Tourism Index were chosen for this analysis.\nSee data collection code and time-series plots below:\n\n\n\n\n\nShow the code\noptions(\"getSymbols.warning4.0\"=FALSE)\noptions(\"getSymbols.yahoo.warning\"=FALSE)\n\n# Collecting Lockheed Martin's Stock Price since IPO\n\ntickers = c(\"LMT\")\nfor (i in tickers){\n  getSymbols(i,\n             from = \"1995-03-17\",\n             to = \"2023-01-30\")}\n\nlmt <- data.frame(LMT$LMT.Adjusted)\n\nlmt <- data.frame(lmt,rownames(lmt))\ncolnames(lmt) <- append(tickers,'Dates')\n\nlmt$date<-as.Date(lmt$Dates,\"%Y-%m-%d\")\n\n# Collecting Raytheon Tech Corp Stock Price since IPO\n\ntickers = c(\"RTX\")\nfor (i in tickers){\n  getSymbols(i,\n             from = \"1983-03-04\",\n             to = \"2023-01-30\")}\n\nrtx <- data.frame(RTX$RTX.Adjusted)\n\nrtx <- data.frame(rtx,rownames(rtx))\ncolnames(rtx) <- append(tickers,'Dates')\n\nrtx$date<-as.Date(rtx$Dates,\"%Y-%m-%d\")\n\n# Collecting DJUSTT since formation\n\ntickers = c(\"^DJUSTT\")\nfor (i in tickers){\n  getSymbols(i,\n             from = \"2008-12-01\",\n             to = \"2023-01-30\")}\n\ndow <- data.frame(DJUSTT$DJUSTT.Adjusted)\n\ndow <- data.frame(dow,rownames(dow))\ncolnames(dow) <- append(tickers,'Dates')\ncolnames(dow)[1] =\"DJUSTT\"\n\ndow$date<-as.Date(dow$Dates,\"%Y-%m-%d\")\n\n\n\nVisualizing Lockheed Martin’s Stock Price and Dow Jones U.S. Travel & Tourism Index\n\nLockheed MartinRaytheon Technologies (formerly American Appliance Company)Dow Jones U.S. Travel & Tourism Index\n\n\n\n\nShow the code\ng1 <- ggplot(lmt, aes(x=date, y=LMT)) +\n  geom_line(color=\"#005BAD\") +\n   labs(\n    title = \"Stock Prices for Lockheed Martin\",\n    subtitle = \"From Dec 2008 - January 2023\",\n    x = \"Date\",\n    y = \"Adjusted Closing Prices ($)\") + \n  theme_minimal() \n\nggplotly(g1) %>%\n  layout(hovermode = \"x\")\n\n\n\n\n\n\n\n\n\n\nShow the code\ng2 <- ggplot(rtx, aes(x=date, y=RTX)) +\n  geom_line(color=\"#E61231\") +\n   labs(\n    title = \"Stock Prices for Raytheon Technologies\",\n    subtitle = \"From Mar 1983 - January 2023\",\n    x = \"Date\",\n    y = \"Adjusted Closing Prices ($)\") + \n  theme_minimal() \n\nggplotly(g2) %>%\n  layout(hovermode = \"x\")\n\n\n\n\n\n\n\n\n\n\nShow the code\ng3 <- ggplot(dow, aes(x=date, y=DJUSTT)) +\n  geom_line() +\n   labs(\n    title = \"Dow Jones U.S. Travel & Tourism Index\",\n    subtitle = \"From March 2020 - December 2022\",\n    x = \"Date\",\n    y = \"Adjusted Closing Prices ($)\") + \n     theme_minimal() \n\nggplotly(g3) %>%\n  layout(hovermode = \"x\")"
  },
  {
    "objectID": "Data-Sources.html#questions-to-adress",
    "href": "Data-Sources.html#questions-to-adress",
    "title": "Data Sources",
    "section": "Questions to Adress",
    "text": "Questions to Adress\n Q1\n Q2\n Q3\n Q4\n Q5\n Q6\n Q7\n Q8\n Q9\n Q10"
  },
  {
    "objectID": "Introduction.html#questions-to-adress",
    "href": "Introduction.html#questions-to-adress",
    "title": "Introduction",
    "section": "Questions to Adress",
    "text": "Questions to Adress\n How have the types of terror attacks in the United States evolved over the last 25 years?\n How have the targets of terror attacks in the United States evolved over the last 25 years?\n Do certain states in the United States suffer more terrorist attacks than others do?\n Are we able to use time-series data on terrorist attacks to predict future attacks and their different kinds in the United States?\n Are we able to use time-series data on military expenditure to predict the government’s future spending in the United States?\n Have certain terrorist attacks on the United States prompted sudden changes in military expenditure over the past 50 years?\n Do terror attacks outside the United States prompt increased domestic military expenditure in the United States?\n What is the correlation between terror attacks and military expenditure over time in the United States?\n To what extent do terror attacks impact the United State’s stance on people entering the United States with B-2 visa (tourist) status?\n To what extent do Lockheed Martin’s stock prices and the Dow Jones U.S. Travel & Tourism Index help us understand trends in terrorist attacks in the United States?"
  },
  {
    "objectID": "Introduction.html#public-us-sentiment",
    "href": "Introduction.html#public-us-sentiment",
    "title": "Introduction",
    "section": "Public US Sentiment",
    "text": "Public US Sentiment\nThe September 11 attacks, commonly known as 9/11, on the World Trade Center in 2001 were a historic aberration in US history, with significant and far-reaching impacts on national security policy, international relations, and the collective psyche of the American people. Immediately after the 9/11 attacks, public sentiment in the US was marked by a strong sense of shock, anger, and a desire for justice, along with a surge in patriotism and a willingness to support government actions to prevent future terrorist attacks. There was also a significant increase in concerns about national security and a greater willingness to sacrifice personal freedoms in the interest of greater security. The figure below conveys that, immediately after 9/11, a share of the US public’s stance on venturing outdoors and travelling overseas stagnated for the next decade. The public’s confidence seemed to restore around 2011."
  },
  {
    "objectID": "Introduction.html#references",
    "href": "Introduction.html#references",
    "title": "Introduction",
    "section": "References",
    "text": "References\nBaker , David. “The Effects of Terrorism on the Travel and Tourism Industry .” Technological University Dublin. International Journal of Religious Tourism and Pilgrimage. Accessed February 1, 2023. https://arrow.tudublin.ie/cgi/viewcontent.cgi?article=1052&context=ijrtp.\n“Codebook Methodology Inclusion Criteria and Ariables - UMD.” Codebook Methodology Inclusion criteria and variables - UMD. University of Maryland. Accessed February 1, 2023. https://www.start.umd.edu/gtd/downloads/Codebook.pdf.\n“World Tourism Organization.” UNWTO. Accessed February 1, 2023. https://www.unwto.org/."
  },
  {
    "objectID": "Data-Visualization.html#visualizing-the-gtd",
    "href": "Data-Visualization.html#visualizing-the-gtd",
    "title": "Data Visualization",
    "section": "Visualizing the GTD™",
    "text": "Visualizing the GTD™\nFor the purpose of this project, the focus will be on the United States, so the data has been filtered accordingly. An important point to keep in mind is that incidents of terrorism from 1993 are not present in the GTD™ because they were lost by the authors. Hence, few visualizations created in R, using the GTD™, do reflect this aberration, as missing values for the year 1993 were not imputed.\nTo start, the visualizations below provide a general overview of how the number of terrorist attacks and fatalities have changed over time.\n\nEvolution of Volume of Terrorist Attacks and Fatalaties (1970-2020) in the US\n\nNumber of Monthly Attacks and FatalitiesNumber of Monthly Attacks and Fatalities (adjusted for 9/11)Cumulative Count of Terrorist Attacks & Fatalities\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe first plot (Number of Monthly Attacks and Fatalities) conveys the significance of the 9/11 Attacks on US history. The big black spike in fatalities, totaling approximately 3000, is representative of the attack and an “outlier” from both series. As a result, in order to depict the trend of both series clearly, it was imperative to filter out the 9/11 Attacks and that is why the second plot was created. A cumulative graph of number of attacks and fatalities is showcased as well that provides further context about the impact of the 9/11 Attacks. Total fatalities were much lower than total number of attakcs from 1970 to 2000, but the toll of the 9/11 Attacks were significant enough to surpass the 2,424 attacks that occurred up until the tragedy.\nMoreover, the total number of fatalities between 1970 and 2000 was 492 and the total number of fatalities between 2001 and 2020 was 419, suggesting that attacks apart from 9/11 follow a similar trend in death rate. Lastly, the number of attacks between the years 1976 and 2004 follow a concave shape, implying that the volume of attacks must be diminishing through the years. However, a steep, exponential rise in not only the number of attacks but also the number of fatalities is noticed after 2004!\nHere are a few facts (“Terrorist Attacks in the u.s Between 1970 and 2013: Data from the ... - DHS,” n.d.) attacks in the US between 1970 and 2013:\n\nApproximately 85% of all deaths from terrorist attacks during this period occurred in the coordinated attacks on September 11, 2001.\nNearly 80% of all terrorist attacks involved no casualties (fatalities or injuries).\nMore than half of terrorist attacks took place during the 1970s. Between 2000 and 2013, there were fewer than 20 attacks per year on average.\n\n\n\nEvolution of Terrorist Attacks: Approach, Victims, and Weapons (Raw Counts)\n\nNumber of Distinct Attacks Over TimeTrends in Attacks Over Time by Victim TypeTrends in Attacks Over Time by Weapon Type\n\n\n\n\n\n\n\n\n   \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n   \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n   \n\n\n\n\n\n\n\n\n\nThe above bar chart races allow for an animated way to display the number of attacks changing over time by the categorical variables, Attack Type, Victim Type, and Weapon Type. The 1970s and 1980s were dominated by Bombing/Explosion Terrorist Attacks in the US, with Facility/Infrastructure Attacks gaining momentum by the end of the 1980s. Many of these bombings were carried out by leftist extremist groups, such as the Weather Underground and the Black Liberation Army, who were motivated by a variety of political and social causes, including opposition to the Vietnam War, racial injustice, and government oppression (Serrano 2008).\n\nSerrano, Richard A. 2008. “The 1970s Bombing Spree.” Los Angeles Times.\n\nRosenau, William. n.d. “Leftist Terrorism in the United States.” Taylor &Amp; Francis. The Journal of Strategic Studies (2013). https://www.tandfonline.com/journals/fjss20.\nOne factor that contributed to the prevalence of domestic bombing attacks during this period was the rise of radical political activism and social unrest. The Vietnam War was a major source of division in American society, and many activists were inspired to use violent tactics in their protests. Additionally, the civil rights movement and the Black Power movement brought attention to issues of racial inequality, and some extremist groups sought to further their agendas through bombings and other violent actions. Another factor was the relative ease with which these groups could obtain explosives and other materials necessary to carry out bombings. Many of the bombs used in these attacks were constructed using readily available materials such as dynamite and pipe bombs, and there were few restrictions on the purchase of these materials at the time (Rosenau, n.d.).\nIn the 1970s and 1980s, a majority of the victims of these attacks included businesses (corporate offices, restaurants, gas stations, bars, cafés, etc.), the government (government building, government member, former members, or events sponsored by political parties, etc.), and private citizens and property (the public in general or attacks in public areas including markets, commercial streets, busy intersections and pedestrian malls) (“Codebook Methodology Inclusion Criteria and Variables - UMD,” n.d.). Moreover, numerous attacks on abortion clinics were conducted in the 1980s and 1990s by anti-abortion activists. These attacks took various forms, including bombings, arson, and other acts of violence, as well as peaceful protests and acts of civil disobedience. Another factor that contributed to the attacks on abortion clinics was the political and legal context of the time. In 1973, the US Supreme Court issued its landmark decision in Roe v. Wade, which established a constitutional right to abortion. This decision was highly controversial and sparked a wave of political and social activism on both sides of the issue.\n\n“Codebook Methodology Inclusion Criteria and Variables - UMD.” n.d. Global Terrorism Database. University of Maryland. https://www.start.umd.edu/gtd/downloads/Codebook.pdf.\nFrom recent years, the data portrays an increase in attacks against both Religious Figures/Institutions and the police. Therefore, terrorists’ aims and agendas have transformed over time as the underlying narrative of a country’s political climate changes. In a smaller sense, to conduct an attack, the US has also suffered from the evolution of weapons used by terrorists. As aforementioned, the 1970s and 1980s experienced bombings as the majority of attacks and the weapons used during that time support this finding. Explosives and incendiaries made up the majority of weapons used in the 1970s and 1980s, with firearms gaining traction. By the late 90s, less use of explosives is seen and a shift to incendiaries, firearms, chemical, and biological weapons becomes prominent.\nHere are some more facts (“Terrorist Attacks in the u.s Between 1970 and 2013: Data from the ... - DHS,” n.d.) related to the bar chart races:\n\n“Terrorist Attacks in the u.s Between 1970 and 2013: Data from the ... - DHS.” n.d. Terrorist Attacks in the U.S Between 1970 and 2013: Data from the Global Terrorism Database (GTD). START Consortium at the University of Maryland. https://www.dhs.gov/sites/default/files/publications/OPSR_TP_TEVUS_Terrorist-Attacks-US_1970-2013_Overview-508.pdf.\n\n94% of attacks against abortion‐related targets were on clinics, while 6% targeted providers or personnel.\n78% of attacks against educational targets were on schools, universities, or\nother buildings, while 22% targeted teachers or other educational personnel.\n73% of attacks against government targets were on government buildings, facilities, or offices, while 27% targeted personnel, public officials, or politicians.\n\n\n\nEvolution of Terrorist Attacks: Approach, Victims, and Weapons (Percent of Total)\n\nPercentage of Distinct Attacks Over TimePercentage of Attacks Over Time by Victim TypePercentage of Attacks Over Time by Weapon Type\n\n\n\n\n\n\n\n\n   \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n   \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n   \n\n\n\n\n\n\n\n\n\n\n\nEvolution of Terrorist Attacks By US State (Geospatial)"
  },
  {
    "objectID": "Introduction.html#bibliography",
    "href": "Introduction.html#bibliography",
    "title": "Introduction",
    "section": "Bibliography",
    "text": "Bibliography"
  },
  {
    "objectID": "Data-Visualization.html#visualizing-the-sipri-military-expenditure-database",
    "href": "Data-Visualization.html#visualizing-the-sipri-military-expenditure-database",
    "title": "Data Visualization",
    "section": "Visualizing the SIPRI Military Expenditure Database",
    "text": "Visualizing the SIPRI Military Expenditure Database"
  },
  {
    "objectID": "Data-Visualization.html#visualizing-department-of-homeland-securitys-non-immigrant-admissions-data",
    "href": "Data-Visualization.html#visualizing-department-of-homeland-securitys-non-immigrant-admissions-data",
    "title": "Data Visualization",
    "section": "Visualizing Department of Homeland Security’s Non-Immigrant Admissions Data",
    "text": "Visualizing Department of Homeland Security’s Non-Immigrant Admissions Data"
  },
  {
    "objectID": "Data-Visualization.html#references",
    "href": "Data-Visualization.html#references",
    "title": "Data Visualization",
    "section": "References",
    "text": "References\n“Terrorist Attacks in the U.S between 1970 and 2013: Data from the … - DHS.” Terrorist Attacks in the U.S Between 1970 and 2013: Data from the Global Terrorism Database (GTD). START Consortium at the University of Maryland. Accessed February 12, 2023. https://www.dhs.gov/sites/default/files/publications/OPSR_TP_TEVUS_Terrorist-Attacks-US_1970-2013_Overview-508.pdf.\nSerrano, Richard A. “The 1970s Bombing Spree.” Los Angeles Times. May 19, 2008.\nRosenau, William. “Leftist Terrorism in the United States.” Taylor & Francis. Accessed February 12, 2023. https://www.tandfonline.com/journals/fjss20.\n“Codebook Methodology Inclusion Criteria and Ariables - UMD.” Codebook Methodology Inclusion criteria and variables - UMD. University of Maryland. Accessed February 1, 2023. https://www.start.umd.edu/gtd/downloads/Codebook.pdf."
  },
  {
    "objectID": "Exploratory-Data-Analysis.html",
    "href": "Exploratory-Data-Analysis.html",
    "title": "Exploratory Data Analysis",
    "section": "",
    "text": "After producing the data visualizations to gain rudimentary insights about the various datasets collected, the next step of the process is to complete an exploratory data analysis (EDA). Several time series packages exist in the R software that have been utilized to unravel deeper details about the data sets. Some of the famous time series analysis methods used in this section include decomposing and identifying time series components, producing auto-correlation function (ACF) and partial auto-correlation function (PACF) plots, and differencing, and checking for stationarity by the use of the Augmented Dickey-Fuller Test.\nCode for this section can be found here"
  },
  {
    "objectID": "Exploratory-Data-Analysis.html#identifying-time-series-components-of-monthly-attacks",
    "href": "Exploratory-Data-Analysis.html#identifying-time-series-components-of-monthly-attacks",
    "title": "Exploratory Data Analysis",
    "section": "Identifying Time Series Components of Monthly Attacks",
    "text": "Identifying Time Series Components of Monthly Attacks"
  },
  {
    "objectID": "Exploratory-Data-Analysis.html#identifying-time-series-components-of-monthly-attacks-gtd",
    "href": "Exploratory-Data-Analysis.html#identifying-time-series-components-of-monthly-attacks-gtd",
    "title": "Exploratory Data Analysis",
    "section": "Identifying Time Series Components of Monthly Attacks (GTD)",
    "text": "Identifying Time Series Components of Monthly Attacks (GTD)\nPlease note that as per the GTD Codebook (“Codebook Methodology Inclusion Criteria and Variables - UMD,” n.d.), incidents of terrorism from 1993 are not present because they were lost prior to START’s compilation of the database from multiple data collection efforts. Therefore, monthly attack counts for the year 1993 have been interpolated using the na.approx() function from the zoo library in R. Appendix II of the GTD Codebook provides Country-level statistics for 1993 and for the US, the attack count was 28. However, our interpolated estimates, which took into calculation 1992 and 1994 attack counts, sum up to 54 attacks, which shall be used for EDA.\n\n“Codebook Methodology Inclusion Criteria and Variables - UMD.” n.d. Global Terrorism Database. University of Maryland. https://www.start.umd.edu/gtd/downloads/Codebook.pdf.\nAlso, the data analyzed is count data rather than the measure of a metric. Hence, the results from the time series functions used on this data might not seem like “traditional” outputs seen from data used in class.\n\n\n\n\n\n\n\n\n\nFrom the graph, we see an initial downward trend from 1970 to 1972 and an upward trend soon after until 1975. The trend, however, then remains constant until the 2000s. Another upward trend is noticed after 2010 as more attacks were conducted in recent years. Some seasonality is noticed, with more attacks occurring towards the end of Spring (April and May) and end of Fall (August to October), across the whole timeline, but the number of attacks does vary across months, suggesting periodic fluctuations. From these insights, the series does not seem stationary. Moreover, because we cannot identify whether the average length of cycles is longer than the length of a seasonal pattern, the graph is not cyclical. A stationary time series will have no predictable patterns in the long-term, but given the count of attacks now increasing in recent years, one could deduce or forecast patterns in the number of attacks for the next few months ahead from the present (Dec 2020). Lastly, as the time of the series increases, the seasonal variation remains fairly constant, so we should use an additive decomposition. Next, we shall take a look at this series’ lag plots to check for autocorrelations, if any."
  },
  {
    "objectID": "Exploratory-Data-Analysis.html#references",
    "href": "Exploratory-Data-Analysis.html#references",
    "title": "Exploratory Data Analysis",
    "section": "References",
    "text": "References\n“Codebook Methodology Inclusion Criteria and Ariables - UMD.” Codebook Methodology Inclusion criteria and variables - UMD. University of Maryland. Accessed February 1, 2023. https://www.start.umd.edu/gtd/downloads/Codebook.pdf."
  },
  {
    "objectID": "Exploratory-Data-Analysis.html#lag-plots-gtd",
    "href": "Exploratory-Data-Analysis.html#lag-plots-gtd",
    "title": "Exploratory Data Analysis",
    "section": "Lag Plots (GTD)",
    "text": "Lag Plots (GTD)"
  },
  {
    "objectID": "Exploratory-Data-Analysis.html#lag-plots-of-monthly-attacks-gtd",
    "href": "Exploratory-Data-Analysis.html#lag-plots-of-monthly-attacks-gtd",
    "title": "Exploratory Data Analysis",
    "section": "Lag Plots of Monthly Attacks (GTD)",
    "text": "Lag Plots of Monthly Attacks (GTD)\n\n\n\n\n\nConcerning the the faceted lag plots of the monthly series, we see a relatively strong positive autocorrelation at lag 1. Thus indicates that there is a strong relationship between the values of the series in adjacent months. Specifically, it suggests that the value of the series in the current month is positively related to the value of the series in the previous month. This can indicate the presence of some underlying trend or seasonality in the data. There is no evidence of negative autocorrelation too. Therefore, this could make a case for weak autocorrelation. As the level of autocorrelation increases, the points shift away from the diagonal; however, the points move closer at lag 12, indicating that . A positive linear trend (i.e. going upwards from left to right) is suggestive of positive autocorrelation.\nWhen comparing the lag plots for the series with different months, there is not much difference, except for a cluster of data points in the bottom left side of the graph, reinforcing our earlier finding that more than half the attacks from 1970 to 2013 took place in 1970. The trend and seasonal components are very much similar and, hence, the plots hint to us that all the series seem to not be stationary."
  },
  {
    "objectID": "Exploratory-Data-Analysis.html#decomposing-monthly-attacks-gtd",
    "href": "Exploratory-Data-Analysis.html#decomposing-monthly-attacks-gtd",
    "title": "Exploratory Data Analysis",
    "section": "Decomposing Monthly Attacks (GTD)",
    "text": "Decomposing Monthly Attacks (GTD)"
  },
  {
    "objectID": "Exploratory-Data-Analysis.html#acf-and-pacf-plots-of-monthly-attacks-gtd",
    "href": "Exploratory-Data-Analysis.html#acf-and-pacf-plots-of-monthly-attacks-gtd",
    "title": "Exploratory Data Analysis",
    "section": "ACF and PACF Plots of Monthly Attacks (GTD)",
    "text": "ACF and PACF Plots of Monthly Attacks (GTD)\n\n\n\n\n\nThe autocorrelation function (ACF) and partial autocorrelation function (PACF) plots are used to help determine the order of an ARMA model. The ACF plot shows the correlation between the time series and its lagged values, while the PACF plot shows the correlation between the time series and its lagged values after controlling for the effects of any intermediate lagged values.\nBy looking at the ACF, it can be concluded that the series is not Stationary. The dashed blue lines indicate whether the correlations are significantly different from zero. The ACF Plot shows a downward trend in attack counts, with the initial insignificant correlations beginning from lag 24. No clear seasonality is depicted from the ACF plot. If a time series is stationary, its PACF should decline to zero relatively quickly, beyond a certain lag value. On the other hand, if a time series is not stationary, its PACF will show significant autocorrelation for many lag values. The former seems true for these PACF plots, as we see autocorrelations for only lags 1 and 2 in the PACF plot. The PACF does decrease after and stays within the confines of the Confidence Interval, which could mean that it is not significantly different from zero and therefore has no significant correlation with the time series from lag 2 onwards. Therefore, the original series might be weakly stationary!"
  },
  {
    "objectID": "Exploratory-Data-Analysis.html#adf-test-of-monthly-attacks-gtd",
    "href": "Exploratory-Data-Analysis.html#adf-test-of-monthly-attacks-gtd",
    "title": "Exploratory Data Analysis",
    "section": "ADF Test of Monthly Attacks (GTD)",
    "text": "ADF Test of Monthly Attacks (GTD)\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  monthly_attacks_ts\nDickey-Fuller = -7.3335, Lag order = 8, p-value = 0.01\nalternative hypothesis: stationary"
  },
  {
    "objectID": "Exploratory-Data-Analysis.html#detrending-monthly-attacks-gtd",
    "href": "Exploratory-Data-Analysis.html#detrending-monthly-attacks-gtd",
    "title": "Exploratory Data Analysis",
    "section": "Detrending Monthly Attacks (GTD)",
    "text": "Detrending Monthly Attacks (GTD)\n\n\n\nCall:\nlm(formula = monthly_attacks_ts ~ time(monthly_attacks_ts), na.action = NULL)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-9.347 -3.363 -1.524  1.317 59.172 \n\nCoefficients:\n                          Estimate Std. Error t value Pr(>|t|)    \n(Intercept)              376.72017   36.25612   10.39   <2e-16 ***\ntime(monthly_attacks_ts)  -0.18622    0.01817  -10.25   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.617 on 610 degrees of freedom\nMultiple R-squared:  0.1469,    Adjusted R-squared:  0.1455 \nF-statistic:   105 on 1 and 610 DF,  p-value: < 2.2e-16\n\n\n\n\n\nOur trend using OLS was:\n\\(\\hat\\mu_t\\) = 376.72 - 0.18622t\nTherefore, equation of the fit of the underlying stationary process is:\n\\(\\hat{y_t}\\) = \\(x_t\\) + 376.72 - 0.18622t\nThe Detrended series is very much similar to the original series, signaling that differencing could provide a more stationary transformation. The linear model’s \\(R^2\\) value is 0.1469, suggesting that the model captures 15% of the variation in prices. Therefore, a quadratic model or first differencing would perhaps provide a better fit.\nWe then see that, even after detrending, the series contains seasonality, which further reinforces our above point that the linear model does not do very well in capturing the initial decreasing trend and then the increasing trend for recent years. Moreover, the data is not trend stationary, which bolsters the above argument."
  },
  {
    "objectID": "Exploratory-Data-Analysis.html#acf-and-pacf-plots-after-differencing-monthly-attacks-gtd",
    "href": "Exploratory-Data-Analysis.html#acf-and-pacf-plots-after-differencing-monthly-attacks-gtd",
    "title": "Exploratory Data Analysis",
    "section": "ACF and PACF Plots After Differencing Monthly Attacks (GTD)",
    "text": "ACF and PACF Plots After Differencing Monthly Attacks (GTD)\n\n\n\n\n\nFirst order differencing performs better than detrending, so we shall use this series in the next section when building our autoregressive models. However, we also saw from the original PACF plot that the PACF declineS to zero relatively quickly than that of the differenced series. Therefore, the original series could also directly be fitted to the autoregressive models."
  },
  {
    "objectID": "Exploratory-Data-Analysis.html#global-terrorism-database-exploratory-data-analysis",
    "href": "Exploratory-Data-Analysis.html#global-terrorism-database-exploratory-data-analysis",
    "title": "Exploratory Data Analysis",
    "section": "Global Terrorism Database Exploratory Data Analysis",
    "text": "Global Terrorism Database Exploratory Data Analysis\n\nIdentifying Time Series Components of Monthly Attacks\nPlease note that as per the GTD Codebook (“Codebook Methodology Inclusion Criteria and Variables - UMD,” n.d.), incidents of terrorism from 1993 are not present because they were lost prior to START’s compilation of the database from multiple data collection efforts. Therefore, monthly attack counts for the year 1993 have been interpolated using the na.approx() function from the zoo library in R. Appendix II of the GTD Codebook provides Country-level statistics for 1993 and for the US, the attack count was 28. However, our interpolated estimates, which took into calculation 1992 and 1994 attack counts, sum up to 54 attacks, which shall be used for EDA.\n\n“Codebook Methodology Inclusion Criteria and Variables - UMD.” n.d. Global Terrorism Database. University of Maryland. https://www.start.umd.edu/gtd/downloads/Codebook.pdf.\nAlso, the data analyzed is count data rather than the measure of a metric. Hence, the results from the time series functions used on this data might not seem like “traditional” outputs seen from data used in class.\n\n\n\n\n\n\n\n\n\nFrom the graph, we see an initial downward trend from 1970 to 1972 and an upward trend soon after until 1975. The trend, however, then remains constant until the 2000s. Another upward trend is noticed after 2010 as more attacks were conducted in recent years. Some seasonality is noticed, with more attacks occurring towards the end of Spring (April and May) and end of Fall (August to October), across the whole timeline, but the number of attacks does vary across months, suggesting periodic fluctuations. From these insights, the series does not seem stationary. Moreover, because we cannot identify whether the average length of cycles is longer than the length of a seasonal pattern, the graph is not cyclical. A stationary time series will have no predictable patterns in the long-term, but given the count of attacks now increasing in recent years, one could deduce or forecast patterns in the number of attacks for the next few months ahead from the present (Dec 2020). Lastly, as the time of the series increases, the seasonal variation remains fairly constant, so we should use an additive decomposition. Next, we shall take a look at this series’ lag plots to check for autocorrelations, if any.\n\n\nLag Plots of Monthly Attacks\n\n\n\n\n\nConcerning the the faceted lag plots of the monthly series, we see a relatively strong positive autocorrelation at lag 1. Thus indicates that there is a strong relationship between the values of the series in adjacent months. Specifically, it suggests that the value of the series in the current month is positively related to the value of the series in the previous month. This can indicate the presence of some underlying trend or seasonality in the data. There is no evidence of negative autocorrelation too. Therefore, this could make a case for weak autocorrelation. As the level of autocorrelation increases, the points shift away from the diagonal; however, the points move closer at lag 12, indicating that . A positive linear trend (i.e. going upwards from left to right) is suggestive of positive autocorrelation.\nWhen comparing the lag plots for the series with different months, there is not much difference, except for a cluster of data points in the bottom left side of the graph, reinforcing our earlier finding that more than half the attacks from 1970 to 2013 took place in 1970. The trend and seasonal components are very much similar and, hence, the plots hint to us that all the series seem to not be stationary.\n\n\nDecomposing Monthly Attacks\n\n\n\n\n\n\n\nACF and PACF Plots of Monthly Attacks\n\n\n\n\n\nThe autocorrelation function (ACF) and partial autocorrelation function (PACF) plots are used to help determine the order of an ARMA model. The ACF plot shows the correlation between the time series and its lagged values, while the PACF plot shows the correlation between the time series and its lagged values after controlling for the effects of any intermediate lagged values.\nBy looking at the ACF, it can be concluded that the series is not Stationary. The dashed blue lines indicate whether the correlations are significantly different from zero. The ACF Plot shows a downward trend in attack counts, with the initial insignificant correlations beginning from lag 24. No clear seasonality is depicted from the ACF plot. If a time series is stationary, its PACF should decline to zero relatively quickly, beyond a certain lag value. On the other hand, if a time series is not stationary, its PACF will show significant autocorrelation for many lag values. The former seems true for these PACF plots, as we see autocorrelations for only lags 1 and 2 in the PACF plot. The PACF does decrease after and stays within the confines of the Confidence Interval, which could mean that it is not significantly different from zero and therefore has no significant correlation with the time series from lag 2 onwards. Therefore, the original series might be weakly stationary!\n\n\nADF Test of Monthly Attacks\n\\(H_0\\): The time series is non-stationary. In other words, it has some time-dependent structure and does not have constant variance over time.\n\\(H_1\\): The time series is stationary.\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  monthly_attacks_ts\nDickey-Fuller = -7.3335, Lag order = 8, p-value = 0.01\nalternative hypothesis: stationary\n\n\nBecause the p-value from the ADF test is less than \\(\\alpha\\) = 0.05, we cannot reject the null hypothesis and conclude that the time series is non-stationary.\n\n\nDetrending Monthly Attacks\n\n\n\nCall:\nlm(formula = monthly_attacks_ts ~ time(monthly_attacks_ts), na.action = NULL)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-9.347 -3.363 -1.524  1.317 59.172 \n\nCoefficients:\n                          Estimate Std. Error t value Pr(>|t|)    \n(Intercept)              376.72017   36.25612   10.39   <2e-16 ***\ntime(monthly_attacks_ts)  -0.18622    0.01817  -10.25   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.617 on 610 degrees of freedom\nMultiple R-squared:  0.1469,    Adjusted R-squared:  0.1455 \nF-statistic:   105 on 1 and 610 DF,  p-value: < 2.2e-16\n\n\n\n\n\nOur trend using OLS was:\n\\(\\hat\\mu_t\\) = 376.72 - 0.18622t\nTherefore, equation of the fit of the underlying stationary process is:\n\\(\\hat{y_t}\\) = \\(x_t\\) + 376.72 - 0.18622t\nThe Detrended series is very much similar to the original series, signaling that differencing could provide a more stationary transformation. The linear model’s \\(R^2\\) value is 0.1469, suggesting that the model captures 15% of the variation in prices. Therefore, a quadratic model or first differencing would perhaps provide a better fit.\nWe then see that, even after detrending, the series contains seasonality, which further reinforces our above point that the linear model does not do very well in capturing the initial decreasing trend and then the increasing trend for recent years. Moreover, the data is not trend stationary, which bolsters the above argument.\n\n\nACF and PACF Plots After Differencing Monthly Attacks\n\n\n\n\n\nFirst order differencing performs better than detrending, so we shall use this series in the next section when building our autoregressive models. However, we also saw from the original PACF plot that the PACF declineS to zero relatively quickly than that of the differenced series. Therefore, the original series could also directly be fitted to the autoregressive models.\n\n\nSimple Moving Average Smoothing\n\n\n\n\n\n\n\nMoving Average Smoothing with Windowing (2x4)\n\n\n\n\n\nmonthly_attacks_ts_2\nma4\nma2x4\n\n\n\n\n26\nNA\nNA\n\n\n50\n49.75\nNA\n\n\n54\n58.50\n54.125\n\n\n69\n55.50\n57.000\n\n\n61\n55.00\n55.250\n\n\n38\n45.00\n50.000\n\n\n\n\n\n\n\n\nIn this case, m is even, so it is no longer be symmetric. Therefore, when windowing, we are applying a moving average to a moving average. One reason for doing this is to make an even-order moving average symmetric. Here we have employed a centered 4-month moving average followed by a centered 2-month moving average. Although this helps smooth out both seasonal and longer-term trends in the data, we notice some seasonality still being present in the smoothed overlay. Let’s us try to use other moving averaging windows to obtain a more stationary overlay.\n\n\nMoving Average Smoothing with Windowing (2x6)\n\n\n\n\n\nmonthly_attacks_ts_2\nma6\nma2x6\n\n\n\n\n26\nNA\nNA\n\n\n50\nNA\nNA\n\n\n54\n49.66667\nNA\n\n\n69\n54.00000\n51.83333\n\n\n61\n50.50000\n52.25000\n\n\n38\n44.83333\n47.66667\n\n\n\n\n\n\n\n\nThe moving average did smooth out both seasonal and longer-term trends in the monthly time series. Although, we could still do better by further smoothing out longer-term trends by using a centered 8-month moving average, showcased below.\n\n\nMoving Average Smoothing with Windowing (2x8)\n\n\n\n\n\nmonthly_attacks_ts_2\nma8\nma2x8\n\n\n\n\n26\nNA\nNA\n\n\n50\nNA\nNA\n\n\n54\nNA\nNA\n\n\n69\n47.375\nNA\n\n\n61\n46.625\n47.0\n\n\n38\n44.375\n45.5\n\n\n\n\n\n\n\n\nThe above plot employed a centered 8-month moving average followed by a centered 2-month moving average. The output is similar to that of the 2x6-MA. Let’s make our analysis more constrained by fitting 4x3-MA: A centered 3-month moving average repeated four times. This should smooth out both seasonal and shorter-term fluctuations even further, providing the least seasonal moving average out of all the other moving averages applied yet."
  },
  {
    "objectID": "Exploratory-Data-Analysis.html#detrending-monthly-attacks",
    "href": "Exploratory-Data-Analysis.html#detrending-monthly-attacks",
    "title": "Exploratory Data Analysis",
    "section": "Detrending Monthly Attacks",
    "text": "Detrending Monthly Attacks\n\n\n\nCall:\nlm(formula = monthly_attacks_ts ~ time(monthly_attacks_ts), na.action = NULL)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-9.347 -3.363 -1.524  1.317 59.172 \n\nCoefficients:\n                          Estimate Std. Error t value Pr(>|t|)    \n(Intercept)              376.72017   36.25612   10.39   <2e-16 ***\ntime(monthly_attacks_ts)  -0.18622    0.01817  -10.25   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.617 on 610 degrees of freedom\nMultiple R-squared:  0.1469,    Adjusted R-squared:  0.1455 \nF-statistic:   105 on 1 and 610 DF,  p-value: < 2.2e-16\n\n\n\n\n\nOur trend using OLS was:\n\\(\\hat\\mu_t\\) = 376.72 - 0.18622t\nTherefore, equation of the fit of the underlying stationary process is:\n\\(\\hat{y_t}\\) = \\(x_t\\) + 376.72 - 0.18622t\nThe Detrended series is very much similar to the original series, signaling that differencing could provide a more stationary transformation. The linear model’s \\(R^2\\) value is 0.1469, suggesting that the model captures 15% of the variation in prices. Therefore, a quadratic model or first differencing would perhaps provide a better fit.\nWe then see that, even after detrending, the series contains seasonality, which further reinforces our above point that the linear model does not do very well in capturing the initial decreasing trend and then the increasing trend for recent years. Moreover, the data is not trend stationary, which bolsters the above argument.\n\nACF and PACF Plots After Differencing Monthly Attacks\n\n\n\n\n\nFirst order differencing performs better than detrending, so we shall use this series in the next section when building our autoregressive models. However, we also saw from the original PACF plot that the PACF declineS to zero relatively quickly than that of the differenced series. Therefore, the original series could also directly be fitted to the autoregressive models.\n\n\nMoving Average Smoothing"
  },
  {
    "objectID": "ARMA-ARIMA-SARIMA.html",
    "href": "ARMA-ARIMA-SARIMA.html",
    "title": "ARMA/ARIMA/SARIMA Models",
    "section": "",
    "text": "After completing the exploratory data analysis (EDA) phase, the next step is to begin building time series models. In order to do so, one must first choose an appropriate model type, such as an ARMA (AutoRegressive Moving Average) model or one of its variations, including ARIMA (AutoRegressive Integrated Moving Average) or SARIMA (Seasonal AutoRegressive Integrated Moving Average).\nAn ARIMA model is generally notated as ARIMA(p,d,q) where p is the order of the AR process, d is the degree of differencing and q is the order of the MA process. The general equation of the model is given as follows:\n\\(\\phi(B)(1-B)^d x_t = \\delta + \\theta(B) w_t\\), where \\(B\\) is the backshift operator, \\(w_t\\) is the Gaussian white noise process, \\(\\delta\\) is the drift term and \\(\\phi(B)\\) and \\(\\theta(B)\\) correspond to the AR and MA parts respectively.\nLag plots, auto-correlation function (ACF) and partial auto-correlation function (PACF) plots, decomposing the time series, and differencing are all useful techniques that were employed during the EDA phase to help inform the choice of model type and parameters. With a solid understanding of the data and its characteristics, one can begin to develop and refine time series models that can be used for forecasting.\nCode for this section can be found here"
  },
  {
    "objectID": "ARMA-ARIMA-SARIMA.html#global-terrorism-database-time-series-modeling",
    "href": "ARMA-ARIMA-SARIMA.html#global-terrorism-database-time-series-modeling",
    "title": "ARMA/ARIMA/SARIMA Models",
    "section": "Global Terrorism Database Time Series Modeling",
    "text": "Global Terrorism Database Time Series Modeling\n\n\n\n\nACF and PACF Plots of Monthly Attacks\n\n\n\n\n\n\n\nADF Test of Monthly Attacks\n\\(H_0\\): The time series is non-stationary. In other words, it has some time-dependent structure and does not have constant variance over time.\n\\(H_1\\): The time series is stationary.\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  monthly_attacks_ts\nDickey-Fuller = -7.3335, Lag order = 8, p-value = 0.01\nalternative hypothesis: stationary\n\n\nBecause the p-value from the ADF test is less than \\(\\alpha\\) = 0.05, we reject the null hypothesis and conclude that the monthly attacks series is stationary. Although the ADF states that the original series is stationary, the ACF plots, which clearly indicate seasonality and trend, are more reliable than the ADF test. Therefore, it is safe to conclude that the series non-stationary as per the (ACF?) section above.\n\n\nLog-Transformation of Monthly Attacks\n\n\n\n\n\n\n\n\nSimply taking log of the number of monthly attacks does not make it stationary. First-differencing the log number of monthly attacks does, however, make the series stationary and this series should be employed for building our time series model. Keep in mind that because first-differencing was enough to make the series stationary, we do not need to second-difference it, helping us avoid over differencing the number of monthly attacks.\n\n\nADF Test of Log First-Differenced Monthly Attacks\n\\(H_0\\): The time series is non-stationary. In other words, it has some time-dependent structure and does not have constant variance over time.\n\\(H_1\\): The time series is stationary.\n\n\n\n    Augmented Dickey-Fuller Test\n\ndata:  dlx\nDickey-Fuller = -13.177, Lag order = 8, p-value = 0.01\nalternative hypothesis: stationary\n\n\nBecause the p-value from the ADF test is less than \\(\\alpha\\) = 0.05, we reject the null hypothesis and conclude that the log first-differenced monthly attacks series is stationary. Let us now check whether the ACF plots supports this hypothesis.\n\n\nACF and PACF Plots of Log First-Differenced Monthly Attacks\n\n\n\n\n\np values obtained from PACF are 1, 2, 3, 4 q values obtained from ACF are: 1 d (Difference): 1\n\n\nFitting ARIMA(p,d,q)\n\n\n\n\n\n\np\nd\nq\nAIC\nBIC\nAICc\n\n\n\n\n2\n1\n1\n2\n1184.760\n1206.827\n1184.859\n\n\n3\n1\n1\n3\n1187.562\n1214.043\n1187.702\n\n\n6\n2\n1\n2\n1188.993\n1215.473\n1189.132\n\n\n4\n1\n1\n4\n1184.796\n1215.690\n1184.982\n\n\n10\n3\n1\n2\n1184.825\n1215.719\n1185.011\n\n\n7\n2\n1\n3\n1188.316\n1219.210\n1188.502\n\n\n11\n3\n1\n3\n1186.472\n1221.780\n1186.712\n\n\n8\n2\n1\n4\n1188.815\n1224.122\n1189.054\n\n\n12\n3\n1\n4\n1187.936\n1227.657\n1188.236\n\n\n9\n3\n1\n1\n1250.578\n1277.059\n1250.718\n\n\n5\n2\n1\n1\n1265.762\n1287.830\n1265.862\n\n\n1\n1\n1\n1\n1319.040\n1336.694\n1319.106\n\n\n\n\n\n\n Best Model in terms of AIC: \n\n\n  p d q     AIC      BIC     AICc\n2 1 1 2 1184.76 1206.827 1184.859\n\n\n\n Best Model in terms of AICc: \n\n\n  p d q     AIC      BIC     AICc\n2 1 1 2 1184.76 1206.827 1184.859\n\n\n\n Best Model in terms of BIC: \n\n\n  p d q     AIC      BIC     AICc\n2 1 1 2 1184.76 1206.827 1184.859\n\n\nThe best model with the lowest AIC, BIC, and AICc metrics is the ARIMA(1, 1, 2) model. The equation of the model is given by:\n\\(\\begin{equation}(1-B)(1-B^1)y_t = \\delta + (1+\\phi_1B)(1-\\theta_1B-\\theta_2B^2)w_t\\end{equation}\\), where \\((1-B)\\) and \\((1-B^1)\\) are the differencing operators, which represent the first-order difference of the series. \\(y_t\\) is the time series, \\(\\delta\\) is the drift term, \\(\\phi_1\\) and \\(\\theta_1\\), \\(\\theta_2\\) are the parameters of the AR and MA parts, respectively, and \\(w_t\\) is the Gaussian white noise process.\nNote that \\(B\\) is the backshift operator, which shifts the time series back by one period.\n\n\nModel Diagnostics of ARIMA(1,1,2)\n\n\n\n\n\nStandardized Residuals: Essentially stating that if the errors are white noise. The model does look stationary as it captures all the signals and essentially captures the raw white noise.\nACF Of Residuals: Auto-correlation of the residuals. The only q value to inspect is 1.\nQ-Q Plot: The series follows a normal distribution pretty closely as even the tails seem to be on the normal line.\np values of the Ljung-Box statistic: Ideally, we would like to fail to reject the null hypothesis. That is, we would like to see the p-value of the test be greater than 0.05 because this means the residuals for our time series model are independent, which is often an assumption we make when creating a model. Since all lag values greater than 5 have a p-value greater than 0.05, our series is stationary.\n\n\nChecking Model Output of ARIMA(1,1,2) with auto.arima()\n\n\nModel metrics using auto.arima(): \n\n\nSeries: monthly_attacks_ts \nARIMA(1,1,1) \n\nCoefficients:\n         ar1      ma1\n      0.1389  -0.7431\ns.e.  0.0645   0.0432\n\nsigma^2 = 18.69:  log likelihood = -1760.85\nAIC=3527.7   AICc=3527.74   BIC=3540.95\n\nTraining set error measures:\n                    ME     RMSE      MAE  MPE MAPE    MASE       ACF1\nTraining set -0.154373 4.313143 2.764126 -Inf  Inf 0.72581 0.01754544\n\n\n\nModel metrics of ARIMA(1, 1, 2) using Arima(): \n\n\nSeries: dlx \nARIMA(1,1,2) \n\nCoefficients:\n         ar1      ma1     ma2\n      0.0327  -1.8568  0.8579\ns.e.  0.0503   0.0299  0.0292\n\nsigma^2 = 0.4:  log likelihood = -589.26\nAIC=1186.51   AICc=1186.58   BIC=1204.17\n\nTraining set error measures:\n                     ME      RMSE       MAE MPE MAPE      MASE         ACF1\nTraining set 0.05076924 0.6304084 0.5003311 NaN  Inf 0.5417286 -0.007967481\n\n\nFrom the above output, auto.arima() outputted an ARIMA(1,1,1) model, which is slightly simpler than the best model obtained with the Arima() function. In fact, the ARIMA(1,1,1) model was the worst model obtained using the Arima() function and that is noted in the table outputted in (best-fit?). This difference can be expected when using auto.arima() because it is not as reliable as the Arima() when building ARIMA models. The case is different when building SARIMA models, which is covered in the next section.\nThe best model outputted by Arima() performs significantly better than the model outputted by auto.arima() in terms of model selection metrics, including AIC, BIC, and AICc. Moreover, the best model outputted by Arima() also outperforms the model outputted by auto.arima() in terms of training set metrics, including RMSE, MAE, and MAPE. Although the auto.arima() model does well to closely match the best Arima() model by including a differenced order in its model and by choosing the right p or AR order, it does not select an additional MA order that would make the model more robust, as seen by the comparison of metrics. Some points to keep in mind when using these functions is as follows.\nThe auto.arima() function in R uses a stepwise algorithm to search through the space of possible ARIMA models and select the one with the lowest AIC value. While this approach can be computationally efficient and provide a good starting point for model selection, it does not necessarily find the best possible model for a given time series.\nOn the other hand, the Arima() function in R allows you to specify the exact order of the ARIMA model and can be used to fit more complex models, such as those with seasonality, exogenous variables, or other constraints. By specifying the exact order of the model, you have more control over the modeling process and can potentially obtain a better fit to the data.\nIn summary, the auto.arima() function can be a useful tool for quickly identifying a potentially good model, but it is not a substitute for careful model selection and customization using the Arima() function.\n\n\nForecasting ARIMA(1,1,2)\n\n\n\n\n\n$pred\n            Jan        Feb        Mar        Apr        May        Jun\n2021 0.37759011 0.02555481 0.01273564 0.01231300 0.01234339 0.01239033\n2022 0.01272329 0.01277086                                            \n            Jul        Aug        Sep        Oct        Nov        Dec\n2021 0.01243788 0.01248544 0.01253301 0.01258058 0.01262815 0.01267572\n2022                                                                  \n\n$se\n           Jan       Feb       Mar       Apr       May       Jun       Jul\n2021 0.6282655 0.8166695 0.8168888 0.8168890 0.8168890 0.8168890 0.8168890\n2022 0.8168890 0.8168890                                                  \n           Aug       Sep       Oct       Nov       Dec\n2021 0.8168890 0.8168890 0.8168890 0.8168890 0.8168890\n2022                                                  \n\n\nFrom the above graph, we can note that the number of attacks will fluctuate anywhere between 0 to 6 (converting from log data) every month from 2021 to 2022, as per the 95% confidence bound. A spike is noticed for the first observed forecast after which the model does poorly to forecast the rest of the year. Since the ARIMA(1,1,2) is relatively simple, it is not as robust and complex to forecast more than a month into the future. The straight red line forecast after the first forecasted month suggests that 1 attack per month (converting from log data) would take place, an averaged out value. This is expected.\nMoreover, the other suboptimal models outputted by Arima() also give the same forecast, so overlaying them onto the above plot would be redundant. It is, however, pragmatic to check whether the auto.arima() model’s forecast may forecast differently. Let us find out below.\n\n\nForecasting ARIMA(1,1,1) Outputted by auto.arima()\n\n\n\n\n\n$pred\n              Jan          Feb          Mar          Apr          May\n2021  0.160739303 -0.066511129  0.048747789 -0.009617016  0.020030951\n2022  0.010443266  0.010418886                                       \n              Jun          Jul          Aug          Sep          Oct\n2021  0.005063770  0.012712754  0.008897226  0.010893251  0.009943302\n2022                                                                 \n              Nov          Dec\n2021  0.010486717  0.010273121\n2022                          \n\n$se\n           Jan       Feb       Mar       Apr       May       Jun       Jul\n2021 0.7049204 0.7897960 0.8104291 0.8155169 0.8168849 0.8172028 0.8173013\n2022 0.8173278 0.8173277                                                  \n           Aug       Sep       Oct       Nov       Dec\n2021 0.8173181 0.8173267 0.8173267 0.8173278 0.8173276\n2022                                                  \n\n\nLike the previous forecast, we can note that, for the auto.arima() model, the number of attacks will fluctuate anywhere between 0 to 6 every month from 2021 to 2022 (again this is logged data), as per the 95% confidence bound. However, from the above plot, it can be discerned from red line that the auto.arima() model’s forecasts fluctuate more for the initial months’ forecasts than that of the Arima() model’s forecasts, which was smoother for the initial 2 months. Like the Arima() model’s forecasts, the the auto.arima() model’s forecast then lie on a single value, 1 attack per month, as the red line flattens out. Note that the output of the auto.arima() model was ARIMA(1,1,1), so it was expected that the forecast would be slightly less smoother than the ARIMA(1,1,2) model outputted by Arima(). This is attributed to the fact that ARIMA(1,1,2) has an additional Moving-Average term, which help make its forecasts smoother.\n\n\nComparing ARIMA(1,1,2) with Benchmarks\n\n\n\n\n\nBest model metrics: \n\n\nSeries: dlx \nARIMA(1,1,2) \n\nCoefficients:\n         ar1      ma1     ma2\n      0.0327  -1.8568  0.8579\ns.e.  0.0503   0.0299  0.0292\n\nsigma^2 = 0.4:  log likelihood = -589.26\nAIC=1186.51   AICc=1186.58   BIC=1204.17\n\nTraining set error measures:\n                     ME      RMSE       MAE MPE MAPE      MASE         ACF1\nTraining set 0.05076924 0.6304084 0.5003311 NaN  Inf 0.5417286 -0.007967481\n\n\n\nSnaive metrics: \n\n\n                       ME     RMSE       MAE MPE MAPE MASE       ACF1\nTraining set -0.000351788 1.145667 0.9235824 NaN  Inf    1 -0.5112064\n\n\nFrom the above plot, only the Snaive benchmark method’s forecasts seem more plausible compared to that of the ARIMA(1,1,2) model. The forecasts produced from the Snaive benchmark have the greatest amount of fluctuations or seasonality in a higher range of number of monthly attacks. However, the metrics paint a different story. The ARIMA(1,1,2) model’s training error measures are better than those of the Snaive benchmark. There are several reasons for this phenomenon:\nModel Assumptions: The ARIMA model assumes that the data is stationary, which means that the mean and variance of the data do not change over time. If the data violates this assumption, the ARIMA model may not perform well. In contrast, the Snaive model does not assume stationarity, which may make it more robust to non-stationary data.\nParameter Estimation: The ARIMA model has three parameters (p, d, q) that need to be estimated, whereas the Snaive model has only one parameter (the seasonality). It is possible that the parameter estimation process for the ARIMA model was not optimal, leading to suboptimal forecast performance.\nForecast Horizon: The Snaive model may perform better than the ARIMA model for shorter forecast horizons, while the ARIMA model may perform better for longer forecast horizons. This is because the Snaive model assumes that the future values of the time series will be the same as the past values at the same time of year, which may be a reasonable assumption for short forecast horizons, but not for longer ones."
  }
]
{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "---\n",
        "title: \"Deep Learning for Time Series\"\n",
        "format:\n",
        "  html:\n",
        "    page-layout: full\n",
        "    fontsize: 14px\n",
        "---"
      ],
      "id": "a5312356"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Import Libraries\n"
      ],
      "id": "2d8dbc22"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from keras.models import Sequential\n",
        "from keras import layers\n",
        "from tensorflow.keras.optimizers import RMSprop"
      ],
      "id": "26c19dd0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Read in Data\n",
        "\n",
        "Data collected is from 2023 to current.\n"
      ],
      "id": "7c0fa0e8"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "df = pd.read_csv(\"mpi_roof.csv\", encoding= 'unicode_escape')\n",
        "\n",
        "f = open('mpi_roof.csv', encoding= 'unicode_escape')\n",
        "data = f.read()\n",
        "f.close()\n",
        "lines = data.split('\\n')\n",
        "header = lines[0].split(',')\n",
        "lines = lines[1:]\n",
        "\n",
        "#df.head()"
      ],
      "id": "50f28420",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Inspecting the Data\n"
      ],
      "id": "d7ef36d3"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# print(df.columns)\n",
        "# print(\"\\n\\n\")\n",
        "# print(\"Shape of data: \", df.shape)\n",
        "\n",
        "print(header)\n",
        "print(len(lines))"
      ],
      "id": "5f39797a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Parsing the Data\n"
      ],
      "id": "95224209"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# convert all columns to float except date column\n",
        "\n",
        "df.iloc[:, 1:] = df.iloc[:, 1:].apply(pd.to_numeric, errors='coerce').astype(float)\n",
        "\n",
        "# convert date to datetime object (date frequency is 10 minutes)\n",
        "\n",
        "df['Date Time'] = pd.to_datetime(df['Date Time'], format='%d.%m.%Y %H:%M:%S')\n",
        "\n",
        "float_data = np.zeros((len(lines), len(header) - 1))\n",
        "for i, line in enumerate(lines):\n",
        "    values = np.array([float(x) for x in line.split(',')[1:]])\n",
        "    if values.shape == (len(header) - 1,):\n",
        "        float_data[i, :] = values\n",
        "    else:\n",
        "        print(f\"Skipping line {i+1} due to incorrect shape of values\")"
      ],
      "id": "b5597359",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Plotting the temperature timeseries\n"
      ],
      "id": "bf69753e"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "plt.figure(figsize=(12, 6))  # Set the width to 12 inches and height to 6 inches\n",
        "plt.plot(df['Date Time'], df['T (degC)']) # x, y encoding\n",
        "plt.title(\"Temperature (degree celsius) over the full temporal range of data\") # title\n",
        "plt.xlabel(\"Date\") # x-axis label\n",
        "plt.ylabel(\"Temperature (degree celsius)\") # y-axis label\n",
        "plt.show()"
      ],
      "id": "e55c97f0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Plotting the first 10 days of the temperature timeseries\n"
      ],
      "id": "d9ab8aa0"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "plt.figure(figsize=(12, 6))  # Set the width to 12 inches and height to 6 inches\n",
        "plt.plot(df['Date Time'][0:1440,], df['T (degC)'][0:1440,]) # x, y encoding\n",
        "plt.title(\"Temperature (degree celsius) for the first 10 days of data\") # title\n",
        "plt.xlabel(\"Date\") # x-axis label\n",
        "plt.ylabel(\"Temperature (degree celsius)\") # y-axis label\n",
        "plt.show()"
      ],
      "id": "116de4ab",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Normalizing the data\n",
        "\n",
        "Preprocess the data by subtracting the mean of each timeseries and dividing by the standard deviation. I am going to use the first 8,000 timesteps as training data,\n",
        "so compute the mean and standard deviation only on this fraction of the data.\n"
      ],
      "id": "d99e8fb5"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# get only numeric cols\n",
        "\n",
        "#float_data = df.iloc[:, 2] # subset only temp data\n",
        "\n",
        "# convert to numpy array\n",
        "#float_data = float_data.to_numpy()\n",
        "\n",
        "mean = float_data[:8000].mean(axis=0)\n",
        "float_data -= mean\n",
        "std = float_data[:8000].std(axis=0)\n",
        "float_data /= std"
      ],
      "id": "400e32e2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Generator yielding timeseries samples and their targets\n"
      ],
      "id": "0efabbf6"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def generator(data, lookback, delay, min_index, max_index,\n",
        "              shuffle=False, batch_size=128, step=6):\n",
        "    if max_index is None:\n",
        "        max_index = len(data) - delay - 1\n",
        "    i = min_index + lookback\n",
        "    while 1:\n",
        "        if shuffle:\n",
        "            rows = np.random.randint(\n",
        "                min_index + lookback, max_index, size=batch_size)\n",
        "        else:\n",
        "            if i + batch_size >= max_index:\n",
        "                i = min_index + lookback\n",
        "            rows = np.arange(i, min(i + batch_size, max_index))\n",
        "            i += len(rows)\n",
        "        samples = np.zeros((len(rows),\n",
        "                            lookback // step,\n",
        "                            data.shape[-1]))\n",
        "        targets = np.zeros((len(rows),))\n",
        "        for j, row in enumerate(rows):\n",
        "            indices = range(rows[j] - lookback, rows[j], step)\n",
        "            samples[j] = data[indices]\n",
        "            targets[j] = data[rows[j] + delay][1]\n",
        "        yield samples, targets\n",
        "\n",
        "lookback = 1440 # observations go back 10 days\n",
        "step = 6 # Observations will be sampled at one data point per hour.\n",
        "delay = 144 # Targets will be 24 hours in the future.\n",
        "batch_size = 128 # Number of samples per batch\n",
        "\n",
        "train_gen = generator(float_data,\n",
        "                      lookback=lookback,\n",
        "                      delay=delay, \n",
        "                      min_index=0, # We will only draw from the first 200,000 timesteps\n",
        "                      max_index=8000, # We will use the first 8000 timesteps as training data\n",
        "                      shuffle=True, # Shuffle the samples\n",
        "                      step=step, \n",
        "                      batch_size=batch_size)\n",
        "\n",
        "val_gen = generator(float_data, \n",
        "                    lookback=lookback,\n",
        "                    delay=delay,\n",
        "                    min_index=8001,\n",
        "                    max_index=12000,\n",
        "                    step=step,\n",
        "                    batch_size=batch_size)\n",
        "\n",
        "test_gen = generator(float_data,\n",
        "                     lookback=lookback,\n",
        "                     delay=delay,\n",
        "                     min_index=10001,\n",
        "                     max_index=None,\n",
        "                     step=step,\n",
        "                     batch_size=batch_size)\n",
        "\n",
        "# How many steps to draw from val_gen in order to see the entire validation set\n",
        "\n",
        "val_steps = (10000 - 8001 - lookback) // batch_size\n",
        "\n",
        "# How many steps to draw from test_gen in order to see the entire test set\n",
        "\n",
        "test_steps = (len(float_data) - 10001 - lookback) // batch_size"
      ],
      "id": "24c1cb87",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Computing the common-sense baseline MAE\n"
      ],
      "id": "2807496a"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def evaluate_naive_method():\n",
        "    batch_maes = []\n",
        "    for step in range(val_steps):\n",
        "        samples, targets = next(val_gen)\n",
        "        preds = samples[:, -1, 1]\n",
        "        mae = np.mean(np.abs(preds - targets))\n",
        "        batch_maes.append(mae)\n",
        "    print(np.mean(batch_maes))\n",
        "\n",
        "evaluate_naive_method()"
      ],
      "id": "9f181be2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Training and evaluating a densely connected model\n"
      ],
      "id": "cb6079fa"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "model = Sequential()\n",
        "model.add(layers.Flatten(input_shape=(lookback // step, float_data.shape[-1])))\n",
        "model.add(layers.Dense(32, activation='relu'))\n",
        "model.add(layers.Dense(1))\n",
        "model.compile(optimizer='rmsprop', loss='mse', metrics=['mae'])\n",
        "history = model.fit_generator(train_gen,\n",
        "                              steps_per_epoch=500,\n",
        "                              epochs=20,\n",
        "                              validation_data=val_gen,\n",
        "                              validation_steps=val_steps)"
      ],
      "id": "acc8cc2f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Plotting results\n"
      ],
      "id": "21260e55"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "epochs = range(len(loss))\n",
        "plt.figure(figsize=(12, 6))  # Set the width to 12 inches and height to 6 inches\n",
        "plt.plot(epochs, loss, 'bo', label='Training loss') # x, y encoding\n",
        "plt.plot(epochs, val_loss, 'b', label='Validation loss') # x, y encoding\n",
        "plt.title('Training and validation loss') # title\n",
        "plt.xlabel('Epochs') # x-axis label\n",
        "plt.ylabel('Loss') # y-axis label\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "id": "ae28cb3e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Overfitting!\n",
        "\n",
        "# Training and evaluating a GRU-based model\n"
      ],
      "id": "d4fe8598"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Flatten this matrix down.\n",
        "# npa = df.iloc[:, 1:].to_numpy().reshape(-1,1) # Python is smart to recognize whatever dimension you need by using this parameter\n",
        "# print(len(npa))\n",
        "# # # Let's scale the data -- this helps avoid the exploding gradient issue\n",
        "# from sklearn.preprocessing import MinMaxScaler\n",
        "# scale = MinMaxScaler(feature_range=(0,1)) # This is by default.\n",
        "# npa = scale.fit_transform(npa)\n",
        "# print(len(npa))\n",
        "\n",
        "# # Need the data to be in the form [sample, time steps, features (dimension of each element)]\n",
        "# samples = 10 # Number of samples (in past)\n",
        "# steps = 1 # Number of steps (in future)\n",
        "# X = [] # X array\n",
        "# Y = [] # Y array\n",
        "# for i in range(npa.shape[0] - samples):\n",
        "#     X.append(npa[i:i+samples]) # Independent Samples\n",
        "#     Y.append(npa[i+samples][0]) # Dependent Samples\n",
        "\n",
        "model = Sequential()\n",
        "model.add(layers.GRU(32, input_shape=(None, float_data.shape[-1])))\n",
        "model.add(layers.Dense(1))\n",
        "model.compile(optimizer='rmsprop', loss='mse', metrics=['mae'])\n",
        "history = model.fit_generator(train_gen,\n",
        "                              steps_per_epoch=500,\n",
        "                              epochs=20,\n",
        "                              validation_data=val_gen,\n",
        "                              validation_steps=val_steps)\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "epochs = range(len(loss))\n",
        "plt.figure(figsize=(12, 6))  # Set the width to 12 inches and height to 6 inches\n",
        "plt.plot(epochs, loss, 'bo', label='Training loss') # x, y encoding\n",
        "plt.plot(epochs, val_loss, 'b', label='Validation loss') # x, y encoding\n",
        "plt.title('Training and validation loss') # title\n",
        "plt.xlabel('Epochs') # x-axis label\n",
        "plt.ylabel('Loss') # y-axis label\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "id": "3c1e5a3d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Training and evaluating a dropout-regularized GRU-based model\n"
      ],
      "id": "06f24fd8"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "model = Sequential()\n",
        "model.add(layers.GRU(32,\n",
        "                     dropout=0.2,\n",
        "                     recurrent_dropout=0.2,\n",
        "                     input_shape=(None, float_data.shape[-1])))\n",
        "model.add(layers.Dense(1))\n",
        "model.compile(optimizer='rmsprop', loss='mse', metrics=['mae'])\n",
        "history = model.fit_generator(train_gen,\n",
        "                              steps_per_epoch=500,\n",
        "                              epochs=40,\n",
        "                              validation_data=val_gen,\n",
        "                              validation_steps=val_steps)\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "epochs = range(len(loss))\n",
        "plt.figure(figsize=(12, 6))  # Set the width to 12 inches and height to 6 inches\n",
        "\n",
        "plt.plot(epochs, loss, 'bo', label='Training loss') # x, y encoding\n",
        "plt.plot(epochs, val_loss, 'b', label='Validation loss') # x, y encoding\n",
        "plt.title('Training and validation loss') # title\n",
        "plt.xlabel('Epochs') # x-axis label\n",
        "plt.ylabel('Loss') # y-axis label\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "id": "15a399ff",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Training and evaluating a dropout-regularized, stacked GRU model\n"
      ],
      "id": "bca9c5e6"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "model = Sequential()\n",
        "model.add(layers.GRU(32,\n",
        "                     dropout=0.1,\n",
        "                     recurrent_dropout=0.5,\n",
        "                     return_sequences=True,\n",
        "                     input_shape=(None, float_data.shape[-1])))\n",
        "model.add(layers.GRU(64, activation='relu',\n",
        "                      dropout=0.1,\n",
        "                      recurrent_dropout=0.5))\n",
        "model.add(layers.Dense(1))\n",
        "model.compile(optimizer='rmsprop', loss='mse', metrics=['mae'])\n",
        "history = model.fit_generator(train_gen,\n",
        "                              steps_per_epoch=500,\n",
        "                              epochs=40,\n",
        "                              validation_data=val_gen,\n",
        "                              validation_steps=val_steps)\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "epochs = range(len(loss))\n",
        "plt.figure(figsize=(12, 6))  # Set the width to 12 inches and height to 6 inches\n",
        "plt.plot(epochs, loss, 'bo', label='Training loss') # x, y encoding\n",
        "plt.plot(epochs, val_loss, 'b', label='Validation loss') # x, y encoding\n",
        "plt.title('Training and validation loss') # title\n",
        "plt.xlabel('Epochs') # x-axis label\n",
        "plt.ylabel('Loss') # y-axis label\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "id": "0c5a3883",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Using bidirectional RNNs\n",
        "\n",
        "## Training and evaluating an LSTM using reversed sequences\n"
      ],
      "id": "62f0d95e"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from keras.datasets import imdb\n",
        "from keras.preprocessing import sequence\n",
        "from keras import layers\n",
        "from keras.models import Sequential\n",
        "\n",
        "max_features = 10000  # Number of words to consider as features\n",
        "maxlen = 500  # Cuts off texts after this many words (among the max_features most common words)\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
        "\n",
        "x_train = [x[::-1] for x in x_train]\n",
        "x_test = [x[::-1] for x in x_test]\n",
        "\n",
        "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
        "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
        "\n",
        "model = Sequential()\n",
        "model.add(layers.Embedding(max_features, 128))\n",
        "model.add(layers.LSTM(32))\n",
        "model.add(layers.Dense(1, activation='sigmoid'))\n",
        "\n",
        "model.compile(optimizer='rmsprop',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['acc'])\n",
        "history = model.fit(x_train, y_train,\n",
        "                    epochs=10,\n",
        "                    batch_size=128,\n",
        "                    validation_split=0.2)"
      ],
      "id": "cfa61f94",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training and evaluating a bidirectional LSTM\n"
      ],
      "id": "755a38b8"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "model = Sequential()\n",
        "model.add(layers.Embedding(max_features, 32))\n",
        "model.add(layers.Bidirectional(layers.LSTM(32)))\n",
        "model.add(layers.Dense(1, activation='sigmoid'))\n",
        "\n",
        "model.compile(optimizer='rmsprop',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['acc'])\n",
        "\n",
        "history = model.fit(x_train, y_train,\n",
        "                    epochs=10,\n",
        "                    batch_size=128,\n",
        "                    validation_split=0.2)"
      ],
      "id": "2c483052",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training a bidirectional GRU\n"
      ],
      "id": "ed5a5db1"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "model = Sequential()\n",
        "model.add(layers.Bidirectional(\n",
        "    layers.GRU(32), input_shape=(None, float_data.shape[-1])))\n",
        "model.add(layers.Dense(1))\n",
        "\n",
        "model.compile(optimizer='rmsprop', loss='mae')\n",
        "history = model.fit_generator(train_gen,\n",
        "                              steps_per_epoch=500,\n",
        "                              epochs=40,\n",
        "                              validation_data=val_gen,\n",
        "                              validation_steps=val_steps)"
      ],
      "id": "019fa765",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "anly590",
      "language": "python",
      "display_name": "ANLY590"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
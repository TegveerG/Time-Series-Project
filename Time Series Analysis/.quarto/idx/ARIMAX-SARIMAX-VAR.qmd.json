{"title":"ARIMAX/SARIMAX/VAR Models","markdown":{"yaml":{"title":"ARIMAX/SARIMAX/VAR Models","format":{"html":{"page-layout":"full","code-fold":"show","code-copy":true,"code-tools":true,"code-overflow":"wrap"}},"bibliography":"bibliography.bib"},"headingText":"Summary","containsRefs":false,"markdown":"\n\n\nIn the previous ARIMA/SARIMA modeling section, we analyzed a univariate time series of monthly terrorist attacks in the US that occurred from 1970 to 2020. Although the SARIMA model performed better than the ARIMA model, due to the added seasonal Moving Average term, we can gauge our understanding of the monthly terrorist attacks better by including __endogenous variables__! Endogenous variables are those that are determined within the system being studied and are influenced by other variables in the system. These variables are typically modeled as being interdependent and are affected by changes in the values of other variables. In the case of terrorist attacks and casualties suffered from them, potential endogenous variables could include USA military expenditure, non-immigrant admissions data, and the performance of major weapons contracts, including Lockheed Martin, Boeing, and Raytheon Technologies. With the help of a literature review, it can be reinforced thoroughly and plausibly that the aforementioned endogenous variables have, in fact, not only been employed but also found to have effects related to terrorism in prior research.\n\n## Literature Review {#Lit}\n\nThe Stimson Study Group's Report on Counterterrorism Spending [@stimson2018counterterroism] in the post-9/11 era provides valuable insights into the amount of resources the United States devotes to counterterrorism efforts. The report found that the US government spent over USD2.8 trillion on counterterrorism efforts from 2002 to 2017, which represents a significant portion of the country's overall military budget during that period. Specifically, the report notes that counterterrorism spending accounted for 17% to 23% of the US defense budget each year between 2002 and 2017. Hartung (2021) [@hartung_2021] found that the \"Global War on Terror\", which emerged in the early 2000s as a result of the 9/11 attacks, had a significant impact on the political environment, resulting in a surge in the Pentagon's budget, the largest component of the US military budget. This increased funding was largely directed towards __military contractors, Lockheed Martin, Boeing, General Dynamics, Raytheon Technologies, and Northrop Grumman__, who were enlisted to aid in the efforts. Since Fiscal Year 2001, the total expenditures of the Pentagon for all purposes have surpassed USD14.1 trillion (measured in 2021 dollars). Out of this sum, USD4.4 trillion was used for weapons procurement and research and development (R&D), which mainly benefited corporate contractors. __The rest of the funds were utilized for__ paying and providing benefits to military and civilian personnel and other expenses, necessary for __operating and maintaining the United States military__. Congressional Research Service (CRS) estimates that in FY2020, the spending for contractors grew to \\$420 billion - well over half of the total Pentagon budget. Therefore, the biggest financial beneficiaries of the post-9/11 military spending surge have been the aforementioned weapons contractors.\n\nFurthermore, several papers have discussed and analyzed the relation between military spending or counterterrorism efforts with transnational terrorism prior to the 9/11 attacks. Li and Schaub (2004) [@LiandSchaub2004] employed *GOVCAPABILITY*, a control variable in their Pooled Time-Series Analysis , that comprised military manpower and military expenditures for 112 countries from 1975 to 1997. Because the variable captured state military and economic strength, it represented a proxy that the government could use for combating terrorism. Gaibulloev, K., Sandler, T., & Sul, D. (2014) [@gaibulloev_sandler_sul_2014] challenged extant literature about terrorism and its impact on economic growth that suffered from Nickell Bias, a type of bias that arises in statistical models when the independent variable is measured with error, and cross-sectional dependence, a statistical issue that arises in panel data analysis when the individual units (e.g., countries or firms) in the panel are not completely independent of one another. They mentioned that cross-sectional dependence is apt to affect other variables, such as democracy, threat, __military spending__, and financial measures. However, when Nickell bias and cross-sectional dependence are addressed, terrorism has no influence whatsoever on economic growth.\n\nTherefore, the perused literature about counterterrorism and military spending underscores the importance of counterterrorism efforts, including funding weapons contractors, in shaping the country's military spending priorities, particularly in the wake of the 9/11 terrorist attacks. By highlighting the amount of resources devoted to counterterrorism, these reports and papers help us understand how the overall US budget and military budget are allocated and the policy decisions that drive those allocations. However, it might also possible that during the VAR model building phase of this section, we find that military spending may not be a significant indicator of the number of casualties stemming from terrorist attacks in the Global Terrorism Database™ (GTD) [@GTD].\n\nSecondly, Nowrasteh (2019) [@nowrasteh_2019] found, by carefully analyzing the GTD, that the chance of being murdered by a tourist on a B visa, the most common tourist visa, is about 1 in 4.1 million per year. Compared to foreign‐born terrorists, the chance of being murdered by a native‐born terrorist is about 1 in 28 million per year. Moreover, there were 192 foreign‐born terrorists, relative to the 788 native-born terrorists, who planned, attempted, or carried out attacks on U.S. soil from 1975 through 2017. Through a cost-benefit risk analysis, it was also found that the combined human, property, business, and economic costs of terrorism from 1975 through 2017 are estimated at USD216.58 billion. Spread over 43 years, the average annual cost of terrorism is USD5.04 billion, which is about one‐hundredth the minimum estimated yearly benefit of USD553.9 billion from immigration and tourism. __Therefore, foreign‐born terrorism on U.S. soil is a low‐probability event that imposes high costs on its victims, despite relatively small risks.__\n\n## VAR Model Justification\n\nUsing a VAR model over an ARIMAX model for this research has a multitude of benefits, including:\n\n-   Simultaneous modeling of multivariate time series: VAR allows for the simultaneous modeling of multiple endogenous variables, whereas ARIMAX (Autoregressive Integrated Moving Average with Explanatory Variables) can only model one dependent variable at a time. This means that VAR can capture the complex relationships between multiple variables that may be influencing each other.\n\n-   Better handling of lags: VAR can handle multiple lags in the data more efficiently than ARIMAX. This is important in the case of studying the impact of terrorist attacks, as the effects of a single attack may persist over a longer period of time and may have delayed impacts on different variables.\n\n-   More robust to missing data: VAR can handle missing data more effectively than ARIMAX, as it does not require the same level of complete data in order to estimate the model parameters. This is particularly relevant in the case of studying the impact of terrorist attacks, as data may be missing or incomplete for certain variables in certain time periods.\n\n-   Better captures dynamic relationships: VAR is better suited for capturing the dynamic relationships between variables over time, whereas ARIMAX can only capture the static relationships between variables at a particular point in time. This is important in the case of studying the impact of terrorist attacks, as the relationships between variables may change over time due to factors such as changes in government policies or public opinion.\n\n-   No clear seasonality: Another reason why the VAR model is justified is that there may not be a clear seasonality pattern in the data related to terrorist attacks. This means that traditional time-series models may not be effective in capturing the complex relationships between the variables. The VAR model, on the other hand, does not rely on a specific seasonal pattern and can account for the complex relationships between variables without requiring seasonal adjustments. Moreover, the data that will be analyzed is aggregated yearly from 1970 to 2020, which makes it more difficult to identify and capture seasonal patterns. Yearly data may also be influenced by other factors that are not related to seasonality, such as long-term trends or cyclical patterns, that occur over longer periods of time. This can make it more challenging to distinguish between seasonal effects and other underlying factors that may be driving the data.\n\n-   Non-linearity: The relationships between variables in the case of terrorist attacks may not be linear, and traditional linear models may not be able to capture the non-linear effects. The VAR model is capable of modeling non-linear relationships between variables and can account for the complex interactions that may exist between them.\n\n## Methodology\n\nGiven the discussed Literature Review above, we shall divide our analysis into two segments, with VAR and ARIMAX models applied to both. __The first segment shall forecast the number of terrorist attacks in the US using all the data available, from 1970-2020, and the second segment shall forecast the number of terrorist attacks in the US using only Post-9/11 attacks data, from 2001-2020.__ The reason 2001 is included is to ensure that the analysis captures the full impact of the 9/11 attacks, immediately before and after it, on terrorist activity in the US.\n\nAnother important differentiator between both VAR models and ARIMAX models is that only the ARIMAX models shall account for all variables collected for the project, including Non-Immigrant Entrants data from the Department of Homeland Security. This is because, as per the literature review, tourism, which is generalized to foreign-born terrorists in this analysis, does not seem to have a significant impact on the likelihood of terrorist attacks on U.S. soil, compared to overall US Military Expenditure. Therefore, this section will help us prove whether the non-immigration variables have a significant effect on the yearly number of terrorist attacks in the US.\n\nFollowing graphs shall help make sense of the roadmap that follows in this analysis:\n\n![](multivariate_seg1.png)\n\n\\\n&nbsp;\n\n<center><em>Multivariate Analysis (VAR and ARIMAX) Segment 1 Flow Chart</em></center>\n\n\\\n&nbsp;\n\n![](multivariate_seg2.png)\n\n\\\n&nbsp;\n\n<center><em>Multivariate Analysis (VAR and ARIMAX) Segment 2 Flow Chart</em></center>\n\n\\\n&nbsp;\n\n## Building the VAR Model (1970-2020: Pre and Post-9/11 Attacks)\n\n```{r,include=FALSE, message=FALSE, warning=FALSE}\nlibrary(flipbookr)\nlibrary(tidyverse)\nlibrary(ggplot2)\nlibrary(forecast)\nlibrary(astsa) \nlibrary(xts)\nlibrary(tseries)\nlibrary(fpp2)\nlibrary(fma)\nlibrary(lubridate)\nlibrary(tidyverse)\nlibrary(TSstudio)\nlibrary(quantmod)\nlibrary(tidyquant)\nlibrary(plotly)\nlibrary(ggplot2)\nlibrary(padr)\nlibrary(gridExtra)\nlibrary(reshape2)\nlibrary(vars)\nlibrary(glmnet)\nlibrary(kableExtra)\nlibrary(knitr)\n```\n\n```{r load,include=FALSE, message=FALSE, warning=FALSE}\ngtd <- readxl::read_xlsx(\"../Data/gtd.xlsx\")\nsipri_gdp <- readxl::read_xlsx(\"../Data/SIPRI_GDP.xlsx\")\nsipri_region <- readxl::read_xlsx(\"../Data/SIPRI_Region.xlsx\")\ndhs <- readxl::read_xls(\"../Data/DHS_98_21.xls\")\n```\n\n```{r createdate,include=FALSE, message=FALSE, warning=FALSE}\n# if the exact day/month of the event is unknown, this is recorded as “0”\ngtd$Date <- as.Date(with(gtd,paste(iyear,imonth,iday,sep=\"-\")),\"%Y-%m-%d\")\n#gtd$Year <- year(gtd$Date)\n# results in 891 NAs total, 33 of which correspond to country_txt==USA\n```\n\n```{r filterUS,include=FALSE, message=FALSE, warning=FALSE}\n# Filter country_txt==USA\ngtd_USA <- gtd %>%\n            filter(country_txt==\"United States\")\n\n# drop 33 observations from a total of 3121 observations (if taking for '70)\ngtd_USA <- gtd_USA[complete.cases(gtd_USA$Date),]\n\n# impute missing values for nkill (Total Number of Fatalities: victims and attackers) as 0\n\ngtd_USA$nkill[is.na(gtd_USA$nkill)] <- 0\n\n# select desired columns for analysis (num_casualties ~ num_attacks, state, attack_type, weapon_type, victim_type, )\n# gtd_USA <- gtd_USA %>%\n#               dplyr::select(Date, provstate, attacktype1_txt,\n#                       targtype1_txt, nkill, weaptype1_txt)\n```\n\n```{r monthlyagg,include=FALSE, message=FALSE, warning=FALSE}\nlibrary(fastDummies)\n\n# new dataframe for monthly number of attacks 1970-2020, with categorical variables encoded\n\ngtd_USA_monthly <- gtd_USA %>%\n  dplyr::select(Date, provstate, attacktype1_txt, targtype1_txt, nkill, weaptype1_txt) %>%\n  dummy_cols() %>% # convert categorical variables to dummies\n  group_by(year(Date), month(Date)) %>%\n  summarise(num_attacks = n(),\n            num_casualties = sum(nkill),\n            across(starts_with(\"provstate_\"), sum),\n            across(starts_with(\"attacktype1_txt_\"), sum),\n            across(starts_with(\"targtype1_txt_\"), sum),\n            across(starts_with(\"weaptype1_txt_\"), sum))\n\ncolnames(gtd_USA_monthly)[1] =\"Year\"\ncolnames(gtd_USA_monthly)[2] =\"Month\"\n\ngtd_USA_monthly$Date <- as.Date(paste0(gtd_USA_monthly$Year, \"-\", gtd_USA_monthly$Month, \"-01\"), \"%Y-%m-%d\")\ngtd_USA_monthly <- subset(gtd_USA_monthly, select=-c(Year, Month)) # 0 NAs\n\n# Fill missing dates (NAs introduced) -> adding 90 previously missing months more\ngtd_USA_monthly <- gtd_USA_monthly %>%\n              complete(Date = seq.Date(min(Date), max(Date), by=\"month\"))\n```\n\n```{r interpolateactual, message=FALSE, warning=FALSE, include=FALSE}\n# interpolating all columns except date\n\n# fill in NAs with 0 for all years except 1993 for all columns but Date\n\n## create a vector of column names, excluding the first column\n# col_names <- names(gtd_USA_monthly)[-1]\n# #\n# # # loop over each column (except date) and replace NAs with 0 for all years except 1993\n# for (col_name in col_names) {\n#   gtd_USA_monthly[[col_name]] <- ifelse(year(gtd_USA_monthly$Date) != 1993 & is.na(gtd_USA_monthly[[col_name]]), 0, gtd_USA_monthly[[col_name]])\n#  }\n# #\n# # # impute num_attacks\n# gtd_USA_monthly$num_attacks[277:288] <- c(1, 2, 3, 4, 2, 1, 3, 2, 2, 3, 2, 3)\n# #\n# # # impute num_casualties\n# gtd_USA_monthly$num_casualties[277:288] <- c(1, 6, 3, 1, 1, 0, 2, 2, 1, 0, 1, 3)\n# #\n# # # impute counts for categorical variables\n# #\n# # # January: on January 25, 1993, Mir Aimal Kasi, a Pakistani citizen, conducted an armed assault near the entrance of CIA HQs in Virginia\n# gtd_USA_monthly[277, 4:57] <- list(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0)\n# gtd_USA_monthly[277, 58:66] <- list(1, 0, 0, 0, 0, 0, 0, 0, 0)\n# gtd_USA_monthly[277, 67:88] <- list(0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,0,0,0)\n# gtd_USA_monthly[277, 89:100] <- list(0, 0, 0, 0, 1, 0, 0, 0, 0,0,0,0)\n# #\n# # # February: The bombing of the New York City World Trade Center on feb 26, 1993, by Ramzi Yousef and his conspirators killed six people\n# gtd_USA_monthly[278, 4:57] <- list(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0)\n# gtd_USA_monthly[278, 58:66] <- list(1, 0, 1, 0, 0, 0, 0, 0, 0)\n# gtd_USA_monthly[278, 67:88] <- list(0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,0,0,0)\n# gtd_USA_monthly[278, 89:100] <- list(0, 0, 1, 0, 1, 0, 0, 0, 0,0,0,0)\n# #\n# # # March - Dec random imputation based on total attacks monthly\n# #\n# gtd_USA_monthly[279, 4:57] <- list(0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0)\n# gtd_USA_monthly[279, 58:66] <- list(1, 0, 1, 0, 0, 1, 0, 0, 0)\n# gtd_USA_monthly[279, 67:88] <- list(0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,0,0,0)\n# gtd_USA_monthly[279, 89:100] <- list(0, 0, 1, 0, 1, 0, 1, 0, 0,0,0,0)\n# #\n# gtd_USA_monthly[280, 4:57] <- list(0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0)\n# gtd_USA_monthly[280, 58:66] <- list(2, 0, 1, 0, 0, 1, 0, 0, 0)\n# gtd_USA_monthly[280, 67:88] <- list(0, 0, 2, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,0,0,0)\n# gtd_USA_monthly[280, 89:100] <- list(0, 0, 2, 0, 1, 0, 1, 0, 0,0,0,0)\n# #\n# gtd_USA_monthly[281, 4:57] <- list(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0)\n# gtd_USA_monthly[281, 58:66] <- list(1, 0, 1, 0, 0, 0, 0, 0, 0)\n# gtd_USA_monthly[281, 67:88] <- list(0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,0,0,0)\n# gtd_USA_monthly[281, 89:100] <- list(0, 0, 1, 0, 1, 0, 0, 0, 0,0,0,0)\n# #\n# gtd_USA_monthly[282, 4:57] <- list(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0)\n# gtd_USA_monthly[282, 58:66] <- list(1, 0, 0, 0, 0, 0, 0, 0, 0)\n# gtd_USA_monthly[282, 67:88] <- list(0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,0,0,0)\n# gtd_USA_monthly[282, 89:100] <- list(0, 0, 0, 0, 1, 0, 0, 0, 0,0,0,0)\n# #\n# gtd_USA_monthly[283, 4:57] <- list(0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0)\n# gtd_USA_monthly[283, 58:66] <- list(1, 0, 1, 0, 0, 1, 0, 0, 0)\n# gtd_USA_monthly[283, 67:88] <- list(0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,0,0,0)\n# gtd_USA_monthly[283, 89:100] <- list(0, 0, 1, 0, 1, 0, 1, 0, 0,0,0,0)\n# #\n# gtd_USA_monthly[284, 4:57] <- list(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0)\n# gtd_USA_monthly[284, 58:66] <- list(1, 0, 1, 0, 0, 0, 0, 0, 0)\n# gtd_USA_monthly[284, 67:88] <- list(0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,0,0,0)\n# gtd_USA_monthly[284, 89:100] <- list(0, 0, 1, 0, 1, 0, 0, 0, 0,0,0,0)\n# #\n# gtd_USA_monthly[285, 4:57] <- list(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0)\n# gtd_USA_monthly[285, 58:66] <- list(1, 0, 1, 0, 0, 0, 0, 0, 0)\n# gtd_USA_monthly[285, 67:88] <- list(0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,0,0,0)\n# gtd_USA_monthly[285, 89:100] <- list(0, 0, 1, 0, 1, 0, 0, 0, 0,0,0,0)\n# #\n# gtd_USA_monthly[286, 4:57] <- list(0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0)\n# gtd_USA_monthly[286, 58:66] <- list(1, 0, 1, 0, 0, 1, 0, 0, 0)\n# gtd_USA_monthly[286, 67:88] <- list(0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,0,0,0)\n# gtd_USA_monthly[286, 89:100] <- list(0, 0, 1, 0, 1, 0, 1, 0, 0,0,0,0)\n# #\n# gtd_USA_monthly[287, 4:57] <- list(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0)\n# gtd_USA_monthly[287, 58:66] <- list(1, 0, 1, 0, 0, 0, 0, 0, 0)\n# gtd_USA_monthly[287, 67:88] <- list(0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,0,0,0)\n# gtd_USA_monthly[287, 89:100] <- list(0, 0, 1, 0, 1, 0, 0, 0, 0,0,0,0)\n# #\n# gtd_USA_monthly[288, 4:57] <- list(0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0)\n# gtd_USA_monthly[288, 58:66] <- list(1, 0, 1, 0, 0, 1, 0, 0, 0)\n# gtd_USA_monthly[288, 67:88] <- list(0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,0,0,0)\n# gtd_USA_monthly[288, 89:100] <- list(0, 0, 1, 0, 1, 0, 1, 0, 0,0,0,0)\n# \n# # # convert all variables to numeric except date\n#  gtd_USA_monthly[,2:100] <- lapply(gtd_USA_monthly[,2:100], as.numeric)\n```\n\n```{r autoarimaarimax, message=FALSE, warning=FALSE, include=FALSE}\n# shift casualties column before attacks due to xreg\n\n#select desired column order (date column first, then casualties, then attacks)\n# new_order <- c(1, 3, 2, 4:100)\n# \n# # reorder columns\n# gtd_USA_monthly <- gtd_USA_monthly[, new_order]# create a matrix from the three time series columns\n# \n# # convert to matrix\n# ts_matrix <- as.matrix(gtd_USA_monthly[, 2:100])\n# \n# # convert the matrix to a time series object with a monthly frequency of 12\n# arimax_ts <- ts(ts_matrix, frequency = 12,\n#                  start = c(year(gtd_USA_monthly$Date[1]), month(gtd_USA_monthly$Date[1])))\n# \n# # ARIMAX For num_attacks ~ num_casualties + all 54 US states/territories\n# (fit.state <- auto.arima(arimax_ts[,1],\n#   xreg=arimax_ts[,2:55]))\n# \n# (fit.cat.ny <- auto.arima(arimax_ts[,1],\n#   xreg=arimax_ts[,c(2, 35, 59)]))\n# \n# (fit.cat.ca <- auto.arima(arimax_ts[,1],\n#   xreg=arimax_ts[,c(2, 8, 56:99)]))\n\n# ts_matrix <- as.matrix(gtd_dhs_sipri[, 2:7])\n# \n# # convert the matrix to a time series object with a yearly frequency \n# arimax_ts <- ts(ts_matrix, frequency = 1,\n#                  start = 1970)\n# \n# # ARIMAX For num_attacks ~ num_casualties + all 54 US states/territories\n# (fit <- auto.arima(arimax_ts[,1],\n#   xreg=arimax_ts[,2:6]))\n```\n\n```{r cleanvar,include=FALSE, message=FALSE, warning=FALSE}\n# Filter country_txt==USA\ngtd_USA_2 <- gtd %>% \n            filter(country_txt==\"United States\")\n\n# drop 33 observations from a total of 3121 observations (if taking for '70)\ngtd_USA_2 <- gtd_USA_2[complete.cases(gtd_USA_2$Date),]\n\n# impute missing values for nkill (Total Number of Fatalities: victims and attackers) as 0\n\ngtd_USA_2$nkill[is.na(gtd_USA_2$nkill)] <- 0\n\n# select desired columns for analysis (num_casualties ~ num_attacks, state, attack_type, weapon_type, victim_type, )\ngtd_USA_2 <- subset(gtd_USA_2, select=c(Date, nkill))\n\n# new dataframe for monthly number of attacks 1970-2020\ngtd_yearly_attacks_deaths <- gtd_USA_2 %>% \n              group_by(year(Date)) %>% \n                  summarise(nkill=sum(nkill),\n                            num_attacks = n())\n\ncolnames(gtd_yearly_attacks_deaths)[1] =\"Year\"\ncolnames(gtd_yearly_attacks_deaths)[2] =\"num_fatal\"\ncolnames(gtd_yearly_attacks_deaths)[3] =\"num_attacks\"\n\ngtd_yearly_attacks_deaths$Year <- as.Date(paste0(gtd_yearly_attacks_deaths$Year, \"-12-31\"))\n\n# Fill missing dates (0 attacks for those dates)\ngtd_yearly_attacks_deaths <- gtd_yearly_attacks_deaths %>% \n              complete(Year = seq.Date(min(Year), max(Year), by=\"year\")) \n\n# impute 28 ATTACKS in 1993 and 21 casualties in 1993 as per GTD\ngtd_yearly_attacks_deaths[24,2] <- 28\ngtd_yearly_attacks_deaths[24,3] <- 21\n\n# CLEAN DHS df\n## convert year to date time\ndhs$Year <- as.Date(paste(dhs$Year, \"-12-31\", sep = \"\"), format = \"%Y-%m-%d\")\n\n## subset\ndhs <- subset(dhs, select = c(Year, Temporaryvisitorsforpleasure, Temporaryvisitorsforbusiness, Students))\n\n# join with aggregated GTD df \ngtd_dhs <- merge(gtd_yearly_attacks_deaths, dhs, by = \"Year\", all.x = TRUE)\n\n# interpolate NAs in DHS columns (1970 to 1980, 1982 to 84, 86 to 88)\ngtd_dhs[,4] <- imputeTS::na.interpolation(gtd_dhs[,4])\ngtd_dhs[,5] <- imputeTS::na.interpolation(gtd_dhs[,5])\ngtd_dhs[,6] <- imputeTS::na.interpolation(gtd_dhs[,6])\n\n# join sipri dataset -> military expenditure as % of GDP\nmilexp.gdp <- sipri_gdp %>% filter(Country=='United States of America')\nmilexp.gdp <- melt(milexp.gdp, id.vars = 'Country', variable.name = 'Year', value.name = 'GDP') \nmilexp.gdp <- as.numeric(milexp.gdp[22:72, 3])\ngtd_dhs_sipri <- cbind(gtd_dhs, milexp.gdp) \n#gtd_dhs_sipri[32,2] <- 10 # subtracting 3004 number of casualties (9/11 attacks -> outlier event)\n#gtd_dhs_sipri[32,3] <- 43 # subtracting 4 attacks (9/11 attacks -> outlier event)\n\n# Collecting Raytheon Tech Stock Price (only one active since 70's)\n\noptions(\"getSymbols.warning4.0\"=FALSE)\noptions(\"getSymbols.yahoo.warning\"=FALSE)\n\ntickers = c(\"RTX\")\nfor (i in tickers){\n  getSymbols(i,\n             from = \"1970-01-01\",\n             to = \"2021-12-31\")}\n\nrtx <- data.frame(RTX$RTX.Adjusted)\n\nrtx <- data.frame(rtx,rownames(rtx))\ncolnames(rtx) <- append(tickers,'Dates')\n\nrtx$date<-as.Date(rtx$Dates,\"%Y-%m-%d\")\n\nrtx_yearly <- rtx %>% \n  filter(format(date, \"%m-%d\") == \"12-31\") %>% \n  group_by(Year=year(date)) %>% \n  summarise(RTX = last(RTX))\n\nrtx_yearly$Year <- as.Date(paste0(rtx_yearly$Year, \"-12-31\"))\n\n# Fill missing dates \nrtx_yearly <- rtx_yearly %>% \n              complete(Year = seq.Date(min(Year), max(Year), by=\"year\")) \n\nrtx_yearly$RTX <- imputeTS::na.interpolation(rtx_yearly$RTX)\n\n# final join to create final VAR dataset\n\ngtd_dhs_sipri_rtx <- cbind(gtd_dhs_sipri, rtx_yearly$RTX) \n\n# rename cols\n\ncolnames(gtd_dhs_sipri_rtx)[c(4, 5, 8)] <- c(\"Pleasure\", \"Business\", \"RTX\")\n\n# convert df to matrix\n\nts_matrix <- as.matrix(gtd_dhs_sipri_rtx[, 3:8])\n\n# convert the matrix to a time series object with a yearly frequency \nvar_ts <- ts(ts_matrix, frequency = 1,\n                 start = 1970)\n```\n\n### Time Series Plots\n\n```{r tsplots, message=FALSE, warning=FALSE, fig.height=7}\nplot.ts(var_ts , main = \"\", xlab = \"\")\n```\n\n### Pair Plots\n\n```{r pairs, message=FALSE, warning=FALSE, fig.height=10}\n# create scatterplot matrix using plotly\nfig <- plot_ly(\n  data = as.data.frame(var_ts),\n  type = \"splom\",\n  diagonal = list(visible = FALSE),\n  dimensions = list(\n    list(label = \"# Attacks\", values = ~num_attacks),\n    list(label = \"B-2 Visa\", values = ~Pleasure),\n    list(label = \"B-1 Visa\", values = ~Business),\n    list(label = \"F-1 Visa\", values = ~Students),\n    list(label = \"Military Exp\", values = ~milexp.gdp),\n    list(label = \"RTX\", values = ~RTX)\n  )\n) %>%\n  layout(hovermode = \"x\")\n\n\n\n# customize layout\nfig <- fig %>% \n  layout(\n    title = \"Scatterplot Matrix of VAR Model Variables (Pre and Post-9/11)\",\n    xaxis = list(title = \"\"),\n    yaxis = list(title = \"\")\n  )\n\n# display plot\nfig\n```\n\n```{r corr, message=FALSE, warning=FALSE}\n#gtd_dhs_sipri_rtx <- subset(gtd_dhs_sipri_rtx, select=-c(Business, Pleasure, Students))\n\n# convert df back to matrix\n\nts_matrix <- as.matrix(gtd_dhs_sipri_rtx[, 3:5])\n\n# convert the matrix to a time series object with a yearly frequency\nvar_ts <- ts(ts_matrix, frequency = 1,\n                 start = 1970)\n\n# split into train and test sets\n\nset.seed(29830)\ntrain_idx <- sample(nrow(var_ts), 0.9 * nrow(var_ts))\ntrain <- var_ts[train_idx, ]\ntest <- var_ts[-train_idx, ]\n\n# Fit Lasso regression model with cross-validation\n# cv_fit <- cv.glmnet(train[, 2], train[, 1], alpha = 1)\n# \n# # Extract selected variables\n# cv_fits <- as.data.frame(as.matrix(coef(cv_fit)))\n# to_include <- rownames(cv_fits)[cv_fits$s1 != 0]\n```\n\n### Fitting VAR Model\n\nHere we use the `VARselect()` function to find the best `p` to fit `VAR(p)`. We will choose a maximum lag of 10 and check which `p` value returns lowest AIC.\n\n```{r varsel, message=FALSE, warning=FALSE}\n(var_result <- VARselect(var_ts, lag.max = 10, type = \"both\"))\n```\n\nNow, we will fit VAR(1), VAR(2), and VAR(3):\n\nVAR(1) output:\n\n```{r varsel1, message=FALSE, warning=FALSE}\nsummary(fit <- VAR(var_ts, p=1, type=\"both\"))\n```\n\nVAR(2) output:\n\n```{r varsel2, message=FALSE, warning=FALSE}\nsummary(fit <- VAR(var_ts, p=2, type=\"both\"))\n```\n\nVAR(3) output:\n\n```{r varsel3, message=FALSE, warning=FALSE}\nsummary(fit <- VAR(var_ts, p=3, type=\"both\"))\n```\n\n### K-Fold Cross Validation and Model Diagnostics\n\n```{r crossval, message=FALSE, warning=FALSE}\n\n# Define the number of folds for cross-validation\nk <- 5\n\n# Define the p values to test\np_values <- c(1, 2, 3)\n\n# Split the data into k folds\ncv_folds <- cut(seq(1, nrow(var_ts)), breaks = k, labels = FALSE)\n\n# Initialize vectors to store RMSE and AIC values for each p value\nrmse_vec <- numeric(length(p_values))\naic_vec <- numeric(length(p_values))\n\n# Loop over p values and perform cross-validation\nfor (i in seq_along(p_values)) {\n  p <- p_values[i]\n  rmse_cv <- numeric(k)\n  aic_cv <- numeric(k)\n  for (j in 1:k) {\n    # Split the data into training and testing sets\n    train <- var_ts[cv_folds != j, ]\n    test <- var_ts[cv_folds == j, ]\n    \n    # Fit the VAR model with the current p value\n    var_fit <- VAR(train, p = p)\n    \n    # Make predictions for the testing set\n    pred <- predict(var_fit, n.ahead = nrow(test))$fcst\n    \n    # Calculate RMSE and AIC for the current fold\n    rmse_cv[j] <- sqrt(mean((pred$num_attacks - test[,1])^2))\n    aic_cv[j] <- AIC(var_fit)\n  }\n  # Calculate the mean RMSE and AIC across all folds for the current p value\n  rmse_vec[i] <- mean(rmse_cv)\n  aic_vec[i] <- mean(aic_cv)\n}\n\n# Create a table of RMSE and AIC values for each p value\nresults_table <- tibble(p_values, rmse_vec, aic_vec)\n\n# Print the results table\nkable(results_table, format = \"markdown\", \n        col.names = c(\"P Values\", \"Mean RMSE (5 Folds)\", \"Mean AIC (5 Folds)\"), align = \"c\", digits = 2\n        )\n```\n\nThe `VAR(1)` model outputs the lowest Mean RMSE of `r results_table$rmse_vec[1]` attacks from the 5-fold cross validation. However, it has the highest AIC score. Because test set performance is best and it is the simplest model, we shall choose the `VAR(1)` model as the best option.\n\n### Forecasting Chosen Model (p=1)\n\n```{r forc2, message=FALSE, warning=FALSE}\nfinal_var <- VAR(var_ts, p = 1)\n\n(fit.pr = predict(final_var, n.ahead = 5, ci = 0.95)) # 5 years ahead \n\nfanchart(fit.pr) # plot prediction + error\n```\n\nThe above plot showcases the forecasts for each variable present in the `VAR(1)` model, `Number of Yearly Attacks`, `US Military Expenditure as a % of US GDP`, and Closing Price of `Raytheon Technologies Stock`. The predicted forecast, from the years 2021 to 2025, for `Number of Yearly Attacks` is a good sign for the US due to the decreasing and plateauing trend, although the actual observations from 2015 onward suggest an increasing trend. The same forecast trend is discerned for `US Military Expenditure as a % of US GDP`, with `Raytheon Technologies Stock` being the only variable with a predicted increasing trend.\n\nLet us visualize more closely the forecasts for the `Number of Yearly Attacks` from 2021 to 2025, corresponding to the VAR(1) model fitted on all years (1970-2020):\n\n```{r forcattacks2, message=FALSE, warning=FALSE}\n# create df of attack forecasts\ndf_fvar_attack <- as.data.frame(fit.pr$fcst$num_attacks)\n# add year column\ndf_fvar_attack$Year <- c(\"2021\", \"2022\", \"2023\", \"2024\", \"2025\")\n(var_plot <- ggplot(data=df_fvar_attack, aes(x=Year, y=fcst, group = 1)) +\n    geom_line(aes(color=\"Forecast\"), linewidth=1) +\n    geom_ribbon(aes(ymin=lower, ymax=upper, fill=\"Confidence Interval\"), alpha=0.1) +\n    labs(title=\"VAR(1) Forecasts for Number of Yearly Attacks from 2021 to 2025\",\n         y=\"Number of Terrorist Attacks\",\n         color=\"\", fill=\"\",\n         caption=\"Data Sources: Global Terrorism Database, SIPRI Military Expenditure Database, Yahoo Finance\") +\n    scale_color_manual(values = c(\"Forecast\"=\"red\")) +\n    scale_fill_manual(values = c(\"95% Confidence Interval\"=\"steelblue\")) +\n    theme_minimal() + \n  theme(plot.caption.position = \"plot\"))\n#ggplotly(var_plot)\n```\n\n## Building the VAR Model (2001-2020: Post-9/11 Attacks)\n\n```{r cleanvar2,include=FALSE, message=FALSE, warning=FALSE}\ntickers = c(\"LMT\")\nfor (i in tickers){\n  getSymbols(i,\n             from = \"2001-09-11\",\n             to = \"2021-12-31\")}\n\nlmt <- data.frame(LMT$LMT.Adjusted)\n\nlmt <- data.frame(lmt,rownames(lmt))\ncolnames(lmt) <- append(tickers,'Dates')\n\nlmt$date<-as.Date(lmt$Dates,\"%Y-%m-%d\")\n\nlmt_yearly <- lmt %>% \n  filter(format(date, \"%m-%d\") == \"12-31\") %>% \n  group_by(Year=year(date)) %>% \n  summarise(LMT = last(LMT))\n\nlmt_yearly$Year <- as.Date(paste0(lmt_yearly$Year, \"-12-31\"))\n\n# Fill missing dates \nlmt_yearly <- lmt_yearly %>% \n              complete(Year = seq.Date(min(Year), max(Year), by=\"year\")) \n\nlmt_yearly$LMT <- imputeTS::na.interpolation(lmt_yearly$LMT)\n\n# filter gtd_hds_sipri_rtx\n\ngtd_dhs_sipri_rtx <- gtd_dhs_sipri_rtx %>% slice(32:n())\n\n# final join to create final VAR dataset\n\ngtd_dhs_sipri_rtx_lmt <- cbind(gtd_dhs_sipri_rtx, lmt_yearly$LMT) \n\n# rename cols\n\ncolnames(gtd_dhs_sipri_rtx_lmt)[9] <- c(\"LMT\")\n\nts_matrix <- as.matrix(gtd_dhs_sipri_rtx_lmt[, c(3,7,8,9)])\n\n# convert the matrix to a time series object with a yearly frequency\nvar_ts <- ts(ts_matrix, frequency = 1,\n                 start = 2001)\n```\n\n### Time Series Plots\n\n```{r tsplots2, message=FALSE, warning=FALSE, fig.height=7}\nplot.ts(var_ts , main = \"\", xlab = \"\")\n```\n\n### Pair Plots\n\n```{r pairs2, message=FALSE, warning=FALSE, fig.height=10}\n# create scatterplot matrix using plotly\nfig <- plot_ly(\n  data = as.data.frame(var_ts), \n  type = \"splom\",\n  diagonal = list(visible = FALSE),\n  dimensions = list(\n    list(label = \"# Attacks\", values = ~num_attacks),\n    list(label = \"Military Exp\", values = ~milexp.gdp),\n    list(label = \"RTX\", values = ~RTX),\n    list(label = \"LMT\", values = ~LMT)\n  )\n) %>%\n  layout(hovermode = \"x\")\n\nfig <- fig %>% \n  layout(\n    title = \"Scatterplot Matrix of VAR Model Variables (Post-9/11)\",\n    xaxis = list(title = \"\"),\n    yaxis = list(title = \"\")\n  )\n\n# display plot\nfig\n```\n\n### Fitting VAR Model\n\nHere we use the `VARselect()` function to find the best `p` to fit `VAR(p)`. We will choose a maximum lag of 10 and check which `p` value returns lowest AIC.\n\n```{r varp, message=FALSE, warning=FALSE}\n(var_result <- VARselect(var_ts, lag.max = 10, type = \"both\"))\n```\n\nNow, we will fit VAR(1), VAR(2), and VAR(3):\n\nVAR(1) output:\n\n```{r varp1, message=FALSE, warning=FALSE}\nsummary(fit <- VAR(var_ts, p=1, type=\"both\"))\n```\n\nVAR(2) output:\n\n```{r varp2, message=FALSE, warning=FALSE}\nsummary(fit <- VAR(var_ts, p=2, type=\"both\"))\n```\n\nVAR(3) output: `System is computationally singular!`\n\n### K-Fold Cross Validation and Model Diagnostics\n\n```{r crossval2, message=FALSE, warning=FALSE}\n\n# Define the number of folds for cross-validation\nk <- 5\n\n# Define the p values to test\np_values <- c(1, 2)\n\n# Split the data into k folds\ncv_folds <- cut(seq(1, nrow(var_ts)), breaks = k, labels = FALSE)\n\n# Initialize vectors to store RMSE and AIC values for each p value\nrmse_vec <- numeric(length(p_values))\naic_vec <- numeric(length(p_values))\n\n# Loop over p values and perform cross-validation\nfor (i in seq_along(p_values)) {\n  p <- p_values[i]\n  rmse_cv <- numeric(k)\n  aic_cv <- numeric(k)\n  for (j in 1:k) {\n    # Split the data into training and testing sets\n    train <- var_ts[cv_folds != j, ]\n    test <- var_ts[cv_folds == j, ]\n    \n    # Fit the VAR model with the current p value\n    var_fit <- VAR(train, p = p)\n    \n    # Make predictions for the testing set\n    pred <- predict(var_fit, n.ahead = nrow(test))$fcst\n    \n    # Calculate RMSE and AIC for the current fold\n    rmse_cv[j] <- sqrt(mean((pred$num_attacks - test[,1])^2))\n    aic_cv[j] <- AIC(var_fit)\n  }\n  # Calculate the mean RMSE and AIC across all folds for the current p value\n  rmse_vec[i] <- mean(rmse_cv)\n  aic_vec[i] <- mean(aic_cv)\n}\n\n# Create a table of RMSE and AIC values for each p value\nresults_table <- tibble(p_values, rmse_vec, aic_vec)\n\n# Print the results table\nkable(results_table, format = \"markdown\", \n        col.names = c(\"P Values\", \"Mean RMSE (5 Folds)\", \"Mean AIC (5 Folds)\"), align = \"c\", digits = 2\n        )\n```\n\nThe `VAR(1)` model outputs the lowest Mean RMSE of `r results_table$rmse_vec[1]` attacks from the 5-fold cross validation. However, it has the highest AIC score. Because predictive performance is best and it is the simplest model, we shall choose the `VAR(1)` model as the best option.\n\n### Forecasting Chosen Model (p=1)\n\n```{r forc, message=FALSE, warning=FALSE}\nfinal_var <- VAR(var_ts, p = 1)\n\n(fit.pr = predict(final_var, n.ahead = 5, ci = 0.95)) # 5 years ahead \n\nfanchart(fit.pr) # plot prediction + error\n```\n\nThe above plot showcases the forecasts for each variable present in the `VAR(1)` model, `Number of Yearly Attacks`, `US Military Expenditure as a % of US GDP`, and Closing Price of `Raytheon Technologies Stock`. The predicted forecast, from the years 2021 to 2025, for `Number of Yearly Attacks` is a good sign for the US due to the decreasing and plateauing trend, although the latest observations from 2015 onwards suggest an increasing trend. The same forecasted trend is discerned for `US Military Expenditure as a % of US GDP`, with `Raytheon Technologies Stock` being the only variable with a predicted increasing trend.\n\nLet us visualize more closely the forecasts for the `Number of Yearly Attacks` from 2021 to 2025, corresponding to the VAR(1) model fitted on 9/11 and post-9/11 years (2001-2020):\n\n```{r forcattacks, message=FALSE, warning=FALSE}\n# create df of attack forecasts\ndf_fvar_attack <- as.data.frame(fit.pr$fcst$num_attacks)\n# add year column\ndf_fvar_attack$Year <- c(\"2021\", \"2022\", \"2023\", \"2024\", \"2025\")\n(var_plot <- ggplot(data=df_fvar_attack, aes(x=Year, y=fcst, group = 1)) +\n    geom_line(aes(color=\"Forecast\"), linewidth=1) +\n    geom_ribbon(aes(ymin=lower, ymax=upper, fill=\"Confidence Interval\"), alpha=0.1) +\n    labs(title=\"VAR(1) Forecasts for Number of Yearly Attacks from 2021 to 2025\",\n         y=\"Number of Terrorist Attacks\",\n         color=\"\", fill=\"\",\n         caption=\"Data Sources: Global Terrorism Database, SIPRI Military Expenditure Database, Yahoo Finance\") +\n    scale_color_manual(values = c(\"Forecast\"=\"red\")) +\n    scale_fill_manual(values = c(\"95% Confidence Interval\"=\"steelblue\")) +\n    theme_minimal() + \n  theme(plot.caption.position = \"plot\"))\n#ggplotly(var_plot)\n```\n\n## Manual ARIMAX Modeling (1970-2020: Pre and Post-9/11 Attacks)\n\nThe ARIMAX model being analyzed in this section is:\n\n<center>Number of yearly terrorist attacks in the US \\~ Number of yearly B-1 visa entrants + Number of yearly B-2 visa entrants + Number of yearly F-1 visa entrants + Yearly US Military Expenditure as Percentage of US GDP + Raytheon Technologies' Last Yearly Closing Price</center>\n\n\\\n&nbsp;\n\nIts output is as follows:\n\n```{r fitresid, message=FALSE, warning=FALSE}\ngtd_USA_2 <- gtd %>% \n            filter(country_txt==\"United States\")\n\n# drop 33 observations from a total of 3121 observations (if taking for '70)\ngtd_USA_2 <- gtd_USA_2[complete.cases(gtd_USA_2$Date),]\n\n# impute missing values for nkill (Total Number of Fatalities: victims and attackers) as 0\n\ngtd_USA_2$nkill[is.na(gtd_USA_2$nkill)] <- 0\n\n# select desired columns for analysis (num_casualties ~ num_attacks, state, attack_type, weapon_type, victim_type, )\ngtd_USA_2 <- subset(gtd_USA_2, select=c(Date, nkill))\n\n# new dataframe for monthly number of attacks 1970-2020\ngtd_yearly_attacks_deaths <- gtd_USA_2 %>% \n              group_by(year(Date)) %>% \n                  summarise(nkill=sum(nkill),\n                            num_attacks = n())\n\ncolnames(gtd_yearly_attacks_deaths)[1] =\"Year\"\ncolnames(gtd_yearly_attacks_deaths)[2] =\"num_fatal\"\ncolnames(gtd_yearly_attacks_deaths)[3] =\"num_attacks\"\n\ngtd_yearly_attacks_deaths$Year <- as.Date(paste0(gtd_yearly_attacks_deaths$Year, \"-12-31\"))\n\n# Fill missing dates (0 attacks for those dates)\ngtd_yearly_attacks_deaths <- gtd_yearly_attacks_deaths %>% \n              complete(Year = seq.Date(min(Year), max(Year), by=\"year\")) \n\n# impute 28 ATTACKS in 1993 and 21 casualties in 1993 as per GTD\ngtd_yearly_attacks_deaths[24,2] <- 28\ngtd_yearly_attacks_deaths[24,3] <- 21\n\n# CLEAN DHS df\n## convert year to date time\ndhs$Year <- as.Date(paste(dhs$Year, \"-12-31\", sep = \"\"), format = \"%Y-%m-%d\")\n\n## subset\ndhs <- subset(dhs, select = c(Year, Temporaryvisitorsforpleasure, Temporaryvisitorsforbusiness, Students))\n\n# join with aggregated GTD df \ngtd_dhs <- merge(gtd_yearly_attacks_deaths, dhs, by = \"Year\", all.x = TRUE)\n\n# interpolate NAs in DHS columns (1970 to 1980, 1982 to 84, 86 to 88)\ngtd_dhs[,4] <- imputeTS::na.interpolation(gtd_dhs[,4])\ngtd_dhs[,5] <- imputeTS::na.interpolation(gtd_dhs[,5])\ngtd_dhs[,6] <- imputeTS::na.interpolation(gtd_dhs[,6])\n\n# join sipri dataset -> military expenditure as % of GDP\nmilexp.gdp <- sipri_gdp %>% filter(Country=='United States of America')\nmilexp.gdp <- melt(milexp.gdp, id.vars = 'Country', variable.name = 'Year', value.name = 'GDP') \nmilexp.gdp <- as.numeric(milexp.gdp[22:72, 3])\ngtd_dhs_sipri <- cbind(gtd_dhs, milexp.gdp) \n#gtd_dhs_sipri[32,2] <- 10 # subtracting 3004 number of casualties (9/11 attacks -> outlier event)\n#gtd_dhs_sipri[32,3] <- 43 # subtracting 4 attacks (9/11 attacks -> outlier event)\n\n# Collecting Raytheon Tech Stock Price (only one active since 70's)\n\noptions(\"getSymbols.warning4.0\"=FALSE)\noptions(\"getSymbols.yahoo.warning\"=FALSE)\n\ntickers = c(\"RTX\")\nfor (i in tickers){\n  getSymbols(i,\n             from = \"1970-01-01\",\n             to = \"2021-12-31\")}\n\nrtx <- data.frame(RTX$RTX.Adjusted)\n\nrtx <- data.frame(rtx,rownames(rtx))\ncolnames(rtx) <- append(tickers,'Dates')\n\nrtx$date<-as.Date(rtx$Dates,\"%Y-%m-%d\")\n\nrtx_yearly <- rtx %>% \n  filter(format(date, \"%m-%d\") == \"12-31\") %>% \n  group_by(Year=year(date)) %>% \n  summarise(RTX = last(RTX))\n\nrtx_yearly$Year <- as.Date(paste0(rtx_yearly$Year, \"-12-31\"))\n\n# Fill missing dates \nrtx_yearly <- rtx_yearly %>% \n              complete(Year = seq.Date(min(Year), max(Year), by=\"year\")) \n\nrtx_yearly$RTX <- imputeTS::na.interpolation(rtx_yearly$RTX)\n\n# final join to create final VAR dataset\n\ngtd_dhs_sipri_rtx <- cbind(gtd_dhs_sipri, rtx_yearly$RTX) \n\n# rename cols\n\ncolnames(gtd_dhs_sipri_rtx)[c(4, 5, 8)] <- c(\"Pleasure\", \"Business\", \"RTX\")\n\n# convert df to matrix\n\nts_matrix <- as.matrix(gtd_dhs_sipri_rtx[, 2:8])\n\n# convert the matrix to a time series object with a yearly frequency \nvar_ts <- ts(ts_matrix, frequency = 1,\n                 start = 1970)\n```\n\n### Regression Summary and Fitting ARIMA to Residuals\n\n```{r fitregcval, message=FALSE, warning=FALSE}\n\nfit.reg <- lm(num_attacks ~ . - num_fatal, data =var_ts)\nsummary(fit.reg)\n\nres.fit<-ts(residuals(fit.reg),star=decimal_date(as.Date(\"1970-01-01\",format = \"%Y-%m-%d\")),frequency = 1)\n\n############## Then look at the residuals ############\nres.fit %>% ggtsdisplay() # no need to difference\n\n#q=1,3 Q=1 , p=1,2, P=1,2\n#write a funtion\nSARIMA.c=function(p1,p2,q1,q2,P1,P2,Q1,Q2,d1,d2,data){\n  \n  temp=c()\n  d=0\n  D=0\n  s=12\n  \n  i=1\n  temp= data.frame()\n  ls=matrix(rep(NA,9*44),nrow=44)\n  \n  \n  for (p in p1:p2)\n  {\n    for(q in q1:q2)\n    {\n      for(P in P1:P2)\n      {\n        for(Q in Q1:Q2)\n        {\n          for(d in d1:d2)\n       \n        {\n          if(p+d+q+P+D+Q<=8)\n          {\n            \n            model<- Arima(data,order=c(p-1,d,q-1),seasonal=c(P-1,D,Q-1))\n            ls[i,]= c(p-1,d,q-1,P-1,D,Q-1,model$aic,model$bic,model$aicc)\n            i=i+1\n            #print(i)\n            \n          }\n          \n        }\n      }\n    }\n    \n  }\n  \n  }\n  temp= as.data.frame(ls)\n  names(temp)= c(\"p\",\"d\",\"q\",\"P\",\"D\",\"Q\",\"AIC\",\"BIC\",\"AICc\")\n  \n  temp\n  \n}\n\n##q=1,3 Q=0 p=1,2 P=0 d=0\n\noutput=SARIMA.c(p1=1,p2=3,q1=1,q2=4,P1=1,P2=3,Q1=1,Q2=2,d1=0,d2=0,data=res.fit)\noutput\n```\n\n### ARIMA(2,0,0) or AR(2) Forecasting\n\n```{r fcast2, message=FALSE, warning=FALSE}\n\noutput[which.min(output$AIC),] \noutput[which.min(output$BIC),]\noutput[which.min(output$AICc),]\n\nset.seed(1234)\n\nmodel_outputar2 <- capture.output(sarima(res.fit, 2,0,0, 0,0,0))\n\ncat(model_outputar2[30:62], model_outputar2[length(model_outputar2)], sep = \"\\n\")\n\narimaModel_1 <- arima(res.fit, order = c(2,0,0))\nforecast1=predict(arimaModel_1, 5)\n# create df with fcast preds and +-1.96 SE for 95% CI Bands\nfarimax_df <- data.frame(\n  Year = 2021:2025,\n  fcst = as.numeric(forecast1$pred),\n  lower = as.numeric(forecast1$pred - 1.96 * forecast1$se),\n  upper = as.numeric(forecast1$pred + 1.96 * forecast1$se)\n)\n\n#plot(forecast1$pred, main = \"ARIMA(2,0,0) Forecast For 5 Years\", xlab = \"Time\", ylab = \"Values\", col = \"red\")\n(arimax_plot <- ggplot(data=farimax_df, aes(x=Year, y=fcst, group = 1)) +\n    geom_line(aes(color=\"Forecast\"), linewidth=1) +\n    geom_ribbon(aes(ymin=lower, ymax=upper, fill=\"Confidence Interval\"), alpha=0.1) +\n    labs(title=\"ARIMA(2,0,0) Forecasts for Number of Yearly Attacks from 2021 to 2025\",\n         y=\"Number of Terrorist Attacks\",\n         color=\"\", fill=\"\",\n         caption=\"Data Sources: Global Terrorism Database, Department of Homeland Security, SIPRI Military Expenditure Database, Yahoo Finance\") +\n    scale_color_manual(values = c(\"Forecast\"=\"red\")) +\n    scale_fill_manual(values = c(\"95% Confidence Interval\"=\"steelblue\")) +\n    theme_minimal() + \n  theme(plot.caption.position = \"plot\", plot.caption = element_text(size=8)))\n```\n\n## Manual ARIMAX Modeling (2001-2020: Post-9/11 Attacks)\n\n\nThe ARIMAX model being analyzed in this section is:\n\n<center>Number of yearly terrorist attacks in the US \\~ Number of yearly B-1 visa entrants + Number of yearly B-2 visa entrants + Number of yearly F-1 visa entrants + Yearly US Military Expenditure as Percentage of US GDP + Raytheon Technologies' Last Yearly Closing Price + Lockheed Martin's Last Yearly Closing Price</center>\n\n\\\n&nbsp;\n\nIts output is as follows:\n\n```{r fitresid2, message=FALSE, warning=FALSE}\ngtd_USA_2 <- gtd %>% \n            filter(country_txt==\"United States\")\n\n# drop 33 observations from a total of 3121 observations (if taking for '70)\ngtd_USA_2 <- gtd_USA_2[complete.cases(gtd_USA_2$Date),]\n\n# impute missing values for nkill (Total Number of Fatalities: victims and attackers) as 0\n\ngtd_USA_2$nkill[is.na(gtd_USA_2$nkill)] <- 0\n\n# select desired columns for analysis (num_casualties ~ num_attacks, state, attack_type, weapon_type, victim_type, )\ngtd_USA_2 <- subset(gtd_USA_2, select=c(Date, nkill))\n\n# new dataframe for monthly number of attacks 1970-2020\ngtd_yearly_attacks_deaths <- gtd_USA_2 %>% \n              group_by(year(Date)) %>% \n                  summarise(nkill=sum(nkill),\n                            num_attacks = n())\n\ncolnames(gtd_yearly_attacks_deaths)[1] =\"Year\"\ncolnames(gtd_yearly_attacks_deaths)[2] =\"num_fatal\"\ncolnames(gtd_yearly_attacks_deaths)[3] =\"num_attacks\"\n\ngtd_yearly_attacks_deaths$Year <- as.Date(paste0(gtd_yearly_attacks_deaths$Year, \"-12-31\"))\n\n# Fill missing dates (0 attacks for those dates)\ngtd_yearly_attacks_deaths <- gtd_yearly_attacks_deaths %>% \n              complete(Year = seq.Date(min(Year), max(Year), by=\"year\")) \n\n# impute 28 ATTACKS in 1993 and 21 casualties in 1993 as per GTD\ngtd_yearly_attacks_deaths[24,2] <- 28\ngtd_yearly_attacks_deaths[24,3] <- 21\n\n# CLEAN DHS df\n## convert year to date time\ndhs$Year <- as.Date(paste(dhs$Year, \"-12-31\", sep = \"\"), format = \"%Y-%m-%d\")\n\n## subset\ndhs <- subset(dhs, select = c(Year, Temporaryvisitorsforpleasure, Temporaryvisitorsforbusiness, Students))\n\n# join with aggregated GTD df \ngtd_dhs <- merge(gtd_yearly_attacks_deaths, dhs, by = \"Year\", all.x = TRUE)\n\n# interpolate NAs in DHS columns (1970 to 1980, 1982 to 84, 86 to 88)\ngtd_dhs[,4] <- imputeTS::na.interpolation(gtd_dhs[,4])\ngtd_dhs[,5] <- imputeTS::na.interpolation(gtd_dhs[,5])\ngtd_dhs[,6] <- imputeTS::na.interpolation(gtd_dhs[,6])\n\n# join sipri dataset -> military expenditure as % of GDP\nmilexp.gdp <- sipri_gdp %>% filter(Country=='United States of America')\nmilexp.gdp <- melt(milexp.gdp, id.vars = 'Country', variable.name = 'Year', value.name = 'GDP') \nmilexp.gdp <- as.numeric(milexp.gdp[22:72, 3])\ngtd_dhs_sipri <- cbind(gtd_dhs, milexp.gdp) \n#gtd_dhs_sipri[32,2] <- 10 # subtracting 3004 number of casualties (9/11 attacks -> outlier event)\n#gtd_dhs_sipri[32,3] <- 43 # subtracting 4 attacks (9/11 attacks -> outlier event)\n\n# Collecting Raytheon Tech Stock Price (only one active since 70's)\n\noptions(\"getSymbols.warning4.0\"=FALSE)\noptions(\"getSymbols.yahoo.warning\"=FALSE)\n\ntickers = c(\"RTX\")\nfor (i in tickers){\n  getSymbols(i,\n             from = \"1970-01-01\",\n             to = \"2021-12-31\")}\n\nrtx <- data.frame(RTX$RTX.Adjusted)\n\nrtx <- data.frame(rtx,rownames(rtx))\ncolnames(rtx) <- append(tickers,'Dates')\n\nrtx$date<-as.Date(rtx$Dates,\"%Y-%m-%d\")\n\nrtx_yearly <- rtx %>% \n  filter(format(date, \"%m-%d\") == \"12-31\") %>% \n  group_by(Year=year(date)) %>% \n  summarise(RTX = last(RTX))\n\nrtx_yearly$Year <- as.Date(paste0(rtx_yearly$Year, \"-12-31\"))\n\n# Fill missing dates \nrtx_yearly <- rtx_yearly %>% \n              complete(Year = seq.Date(min(Year), max(Year), by=\"year\")) \n\nrtx_yearly$RTX <- imputeTS::na.interpolation(rtx_yearly$RTX)\n\n# join \n\ngtd_dhs_sipri_rtx <- cbind(gtd_dhs_sipri, rtx_yearly$RTX) \n\n# LMT \n\ntickers = c(\"LMT\")\nfor (i in tickers){\n  getSymbols(i,\n             from = \"2001-09-11\",\n             to = \"2021-12-31\")}\n\nlmt <- data.frame(LMT$LMT.Adjusted)\n\nlmt <- data.frame(lmt,rownames(lmt))\ncolnames(lmt) <- append(tickers,'Dates')\n\nlmt$date<-as.Date(lmt$Dates,\"%Y-%m-%d\")\n\nlmt_yearly <- lmt %>% \n  filter(format(date, \"%m-%d\") == \"12-31\") %>% \n  group_by(Year=year(date)) %>% \n  summarise(LMT = last(LMT))\n\nlmt_yearly$Year <- as.Date(paste0(lmt_yearly$Year, \"-12-31\"))\n\n# Fill missing dates \nlmt_yearly <- lmt_yearly %>% \n              complete(Year = seq.Date(min(Year), max(Year), by=\"year\")) \n\nlmt_yearly$LMT <- imputeTS::na.interpolation(lmt_yearly$LMT)\n\n# filter gtd_hds_sipri_rtx\n\ngtd_dhs_sipri_rtx <- gtd_dhs_sipri_rtx %>% slice(32:n())\n\n# final join to create final arimax dataset\n\ngtd_dhs_sipri_rtx_lmt <- cbind(gtd_dhs_sipri_rtx, lmt_yearly$LMT) \n\n# rename cols\n\ncolnames(gtd_dhs_sipri_rtx_lmt)[c(4, 5, 8, 9)] <- c(\"Pleasure\", \"Business\", \"RTX\", \"LMT\")\n\n# convert df to matrix\n\nts_matrix <- as.matrix(gtd_dhs_sipri_rtx_lmt[, 2:9])\n\n# convert the matrix to a time series object with a yearly frequency \nvar_ts <- ts(ts_matrix, frequency = 1,\n                 start = 2001)\n```\n\n### Regression Summary and Fitting ARIMA to Residuals\n\n```{r regres, message=FALSE, warning=FALSE}\nfit.reg <- lm(num_attacks ~ . - num_fatal, data = var_ts)\n\nsummary(fit.reg)\n\nres.fit<-ts(residuals(fit.reg),star=decimal_date(as.Date(\"1970-01-01\",format = \"%Y-%m-%d\")),frequency = 1)\n\n############## Then look at the residuals ############\nres.fit %>% ggtsdisplay() # no need to difference\n\n#q=1,3 Q=1 , p=1,2, P=1,2\n#write a funtion\nSARIMA.c=function(p1,p2,q1,q2,P1,P2,Q1,Q2,d1,d2,data){\n  \n  temp=c()\n  d=0\n  D=0\n  s=12\n  \n  i=1\n  temp= data.frame()\n  ls=matrix(rep(NA,9*44),nrow=44)\n  \n  \n  for (p in p1:p2)\n  {\n    for(q in q1:q2)\n    {\n      for(P in P1:P2)\n      {\n        for(Q in Q1:Q2)\n        {\n          for(d in d1:d2)\n       \n        {\n          if(p+d+q+P+D+Q<=8)\n          {\n            \n            model<- Arima(data,order=c(p-1,d,q-1),seasonal=c(P-1,D,Q-1))\n            ls[i,]= c(p-1,d,q-1,P-1,D,Q-1,model$aic,model$bic,model$aicc)\n            i=i+1\n            #print(i)\n            \n          }\n          \n        }\n      }\n    }\n    \n  }\n  \n  }\n  temp= as.data.frame(ls)\n  names(temp)= c(\"p\",\"d\",\"q\",\"P\",\"D\",\"Q\",\"AIC\",\"BIC\",\"AICc\")\n  \n  temp\n  \n}\n\n##q=1,2 Q=0 , p=1,2 P=0 d=0\n\noutput=SARIMA.c(p1=1,p2=3,q1=1,q2=4,P1=1,P2=3,Q1=1,Q2=2,d1=0,d2=0,data=res.fit)\noutput\n```\n\n### ARIMA(0,0,1) or MA(1) Forecasting\n\n```{r fcast, message=FALSE, warning=FALSE}\noutput[which.min(output$AIC),] \noutput[which.min(output$BIC),]\noutput[which.min(output$AICc),]\n\nset.seed(1234)\n\nmodel_outputma2 <- capture.output(sarima(res.fit, 0,0,1,0,0,0))\n\ncat(model_outputma2[115:143], model_outputma2[length(model_outputar2)], sep = \"\\n\")\n\narimaModel_1 <- arima(res.fit, order = c(0,0,1))\nforecast1=predict(arimaModel_1, 5)\n# create df with fcast preds and +-1.96 SE for 95% CI Bands\nfarimax_df <- data.frame(\n  Year = 2021:2025,\n  fcst = as.numeric(forecast1$pred),\n  lower = as.numeric(forecast1$pred - 1.96 * forecast1$se),\n  upper = as.numeric(forecast1$pred + 1.96 * forecast1$se)\n)\n\n#plot(forecast1$pred, main = \"ARIMA(2,0,0) Forecast For 5 Years\", xlab = \"Time\", ylab = \"Values\", col = \"red\")\n(arimax_plot <- ggplot(data=farimax_df, aes(x=Year, y=fcst, group = 1)) +\n    geom_line(aes(color=\"Forecast\"), linewidth=1) +\n    geom_ribbon(aes(ymin=lower, ymax=upper, fill=\"Confidence Interval\"), alpha=0.1) +\n    labs(title=\"ARIMA(0,0,1) Forecasts for Number of Yearly Attacks from 2021 to 2025\",\n         y=\"Number of Terrorist Attacks\",\n         color=\"\", fill=\"\",\n         caption=\"Data Sources: Global Terrorism Database, Department of Homeland Security, SIPRI Military Expenditure Database, Yahoo Finance\") +\n    scale_color_manual(values = c(\"Forecast\"=\"red\")) +\n    scale_fill_manual(values = c(\"95% Confidence Interval\"=\"steelblue\")) +\n    theme_minimal() + \n  theme(plot.caption.position = \"plot\", plot.caption = element_text(size=8)))\n```\n\n## Final Results\n\nThe VAR(1) Model trained on Post-9/11 Attacks data is the best performing model in terms of the predicted forecast. It is the only model that took into account the latter observations in its prediction and predicted based on that increasing trend. Our hypothesis that non-immigrants do not significantly affect the number of yearly terrorist attacks in the US is, therefore, proven through the above analyses. \n\nThe ARIMAX model trained on all the years performed significantly worse than the ARIMAX model trained on only Post-9/11 Attacks years. In fact, when adding in Lockheed Martin's stock price in segment 2 of the analysis (Post-9/11 Attacks), $Adjusted.R^2$ increases by more than 60% from segment 1 of the analysis, which includes all years (1970-2020) and not Lockheed Martin's stock price! Although that is the case, none of the predicted forecasts from the ARIMAX models are not as convincing as the VAR models. Moreover, in the ARIMAX models, all the non-immigration variables collected from DHS are non-significant, apart from Pleasure or B-2 Visa entrants in segment 1 (all years included) of the analysis.\n\nIn summary, the years prior to the 9/11 Attacks have little to no effect on the forecasts for 2021 to 2025. To gain more accurate forecasts from the VAR model, it is imperative to focus on data post-9/11. The variables that do imply high correlation (determining causation is out of scope of this analysis) to forecasting the yearly number of terrorist attacks in the US are US Military Expenditure both directly on armed forces and weapons contractors, Lockheed Martin and Raytheon Technologies.  \n\n## Section Code\n\n**Code for this section can be found [here](https://github.com/TegveerG/Time-Series-Project/blob/main/Time%20Series%20Analysis/ARIMAX-SARIMAX-VAR.qmd)**"},"formats":{"html":{"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"knitr"},"render":{"keep-tex":false,"keep-yaml":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"show","code-overflow":"wrap","code-link":false,"code-line-numbers":false,"code-tools":true,"tbl-colwidths":"auto","merge-includes":true,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","toc":true,"output-file":"ARIMAX-SARIMAX-VAR.html"},"language":{},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.1.251","editor":"visual","theme":["lux","custom.scss"],"toc_float":{"collapsed":false,"smooth_scroll":false},"code-copy":true,"title":"ARIMAX/SARIMAX/VAR Models","bibliography":["bibliography.bib"],"page-layout":"full"},"extensions":{"book":{"multiFile":true}}}}}